<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scrapy框架 | Felix's Blog</title><meta name=keywords content><meta name=description content="简介
什么是框架？
所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。"><meta name=author content="
作者:Felix"><link rel=canonical href=https://canw0916.github.io/en/posts/tech/scrapy%E6%A1%86%E6%9E%B6/><link crossorigin=anonymous href=/assets/css/stylesheet.67971badef1fb278a7dbb01fdd6e1add4dae2ea34bb3a44665b66327fad3f0ab.css integrity="sha256-Z5cbre8fsnin27Af3W4a3U2uLqNLs6RGZbZjJ/rT8Ks=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://canw0916.github.io/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://canw0916.github.io/img/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://canw0916.github.io/img/favicon-32x32.png><link rel=apple-touch-icon href=https://canw0916.github.io/img/apple-touch-icon.png><link rel=mask-icon href=https://canw0916.github.io/img/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://canw0916.github.io/en/posts/tech/scrapy%E6%A1%86%E6%9E%B6/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-7ENSZ7BS0C"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7ENSZ7BS0C")}</script><meta property="og:title" content="Scrapy框架"><meta property="og:description" content="简介
什么是框架？
所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。"><meta property="og:type" content="article"><meta property="og:url" content="https://canw0916.github.io/en/posts/tech/scrapy%E6%A1%86%E6%9E%B6/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-10T15:08:28+08:00"><meta property="article:modified_time" content="2023-02-10T15:08:28+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scrapy框架"><meta name=twitter:description content="简介
什么是框架？
所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"📚文章","item":"https://canw0916.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"🚀 技术","item":"https://canw0916.github.io/en/posts/tech/"},{"@type":"ListItem","position":3,"name":"Scrapy框架","item":"https://canw0916.github.io/en/posts/tech/scrapy%E6%A1%86%E6%9E%B6/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scrapy框架","name":"Scrapy框架","description":"简介 什么是框架？\n所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。\nScrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。\n","keywords":[],"articleBody":"简介 什么是框架？\n所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。\nScrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。\n初期如何学习框架？\n只需要学习框架集成好的各种功能的用法即可！前期切勿钻研框架的源码！\n安装 Linux/mac系统： pip install scrapy（任意目录下） Windows系统： a. pip install wheel（任意目录下） b. 下载twisted文件，下载网址如下： http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted c. 终端进入下载目录，执行 pip install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl 注意：如果该步骤安装出错，则换一个版本的whl文件即可 d. pip install pywin32（任意目录下） e. pip install scrapy（任意目录下） 如果安装好后，在终端中录入scrapy指令按下回车，如果没有提示找不到该指令，则表示安装成功 基本使用 创建项目\nscrapy startproject 项目名称\n项目的目录结构：\nfirstBlood # 项目所在文件夹, 建议用pycharm打开该文件夹 ├── firstBlood # 项目跟目录 │ ├── __init__.py │ ├── items.py # 封装数据的格式 │ ├── middlewares.py # 所有中间件 │ ├── pipelines.py\t# 所有的管道 │ ├── settings.py\t# 爬虫配置信息 │ └── spiders\t# 爬虫文件夹, 稍后里面会写入爬虫代码 │ └── __init__.py └── scrapy.cfg\t# scrapy项目配置信息,不要删它,别动它,善待它. 创建爬虫爬虫文件：\ncd project_name（进入项目目录） scrapy genspider 爬虫文件的名称（自定义一个名字即可） 起始url （例如：scrapy genspider first www.xxx.com） 创建成功后，会在爬虫文件夹下生成一个py的爬虫文件 编写爬虫文件\n理解爬虫文件的不同组成部分\nimport scrapy class FirstSpider(scrapy.Spider): #爬虫名称：爬虫文件唯一标识：可以使用该变量的值来定位到唯一的一个爬虫文件 name = 'first' #无需改动 #允许的域名：scrapy只可以发起百度域名下的网络请求 # allowed_domains = ['www.baidu.com'] #起始的url列表：列表中存放的url可以被scrapy发起get请求 start_urls = ['https://www.baidu.com/','https://www.sogou.com'] #专门用作于数据解析 #参数response：就是请求之后对应的响应对象 #parse的调用次数，取决于start_urls列表元素的个数 def parse(self, response): print('响应对象为：',response) 配置文件修改:settings.py\n不遵从robots协议：ROBOTSTXT_OBEY = False 指定输出日志的类型：LOG_LEVEL = ‘ERROR’ 指定UA：USER_AGENT = ‘Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36’ 运行项目\nscrapy crawl 爬虫名称 ：该种执行形式会显示执行的日志信息（推荐） scrapy crawl 爬虫名称 --nolog：该种执行形式不会显示执行的日志信息（一般不用） 数据解析 注意，如果终端还在第一个项目的文件夹中，则需要在终端中执行cd ../返回到上级目录，在去新建另一个项目。\n新建数据解析项目：\n创建工程：scrapy startproject 项目名称 cd 项目名称 创建爬虫文件：scrapy genspider 爬虫文件名 www.xxx.com 配置文件的修改：settings.py\n不遵从robots协议：ROBOTSTXT_OBEY = False 指定输出日志的类型：LOG_LEVEL = ‘ERROR’ 指定UA：USER_AGENT = ‘Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36’ 编写爬虫文件：spiders/duanzi.py\nimport scrapy class DuanziSpider(scrapy.Spider): name = 'duanzi' # allowed_domains = ['www.xxx.com'] #对首页进行网络请求 #scrapy会对列表中的url发起get请求 start_urls = ['https://ishuo.cn/duanzi'] def parse(self, response): #如何获取响应数据 #调用xpath方法对响应数据进行xpath形式的数据解析 li_list = response.xpath('//*[@id=\"list\"]/ul/li') for li in li_list: # content = li.xpath('./div[1]/text()')[0] # title = li.xpath('./div[2]/a/text()')[0] # # # print(title)#selector的对象，且我们想要的字符串内容存在于该对象的data参数里 #解析方案1： # title = li.xpath('./div[2]/a/text()')[0] # content = li.xpath('./div[1]/text()')[0] # #extract()可以将selector对象中data参数的值取出 # print(title.extract()) # print(content.extract()) #解析方案2： #title和content为列表，列表只要一个列表元素 title = li.xpath('./div[2]/a/text()') content = li.xpath('./div[1]/text()') #extract_first()可以将列表中第0个列表元素表示的selector对象中data的参数值取出 print(title.extract_first()) print(content.extract_first()) 持久化存储 两种方案：\n基于终端指令的持久化存储 基于管道的持久化存储（推荐） 基于终端指令的持久化存储 只可以将parse方法的返回值存储到指定后缀的文本文件中。\n编码流程：\n在爬虫文件中，将爬取到的数据全部封装到parse方法的返回值中\nimport scrapy class DemoSpider(scrapy.Spider): name = 'demo' # allowed_domains = ['www.xxx.com'] start_urls = ['https://ishuo.cn/duanzi'] def parse(self, response): # 如何获取响应数据 # 调用xpath方法对响应数据进行xpath形式的数据解析 li_list = response.xpath('//*[@id=\"list\"]/ul/li') all_data = []#爬取到的数据全部都存储到了该列表中 for li in li_list: title = li.xpath('./div[2]/a/text()').extract_first() content = li.xpath('./div[1]/text()').extract_first() #将段子标题和内容封装成parse方法的返回 dic = { 'title':title, 'content':content } all_data.append(dic) return all_data 将parse方法的返回值存储到指定后缀的文本文件中:\nscrapy crawl 爬虫文件名称 -o duanzi.csv 总结：\n优点：简单，便捷 缺点：局限性强 只可以将数据存储到文本文件无法写入数据库 存储数据文件后缀是指定好的，通常使用.csv 需要将存储的数据封装到parse方法的返回值中 基于管道实现持久化存储 优点：极大程度的提升数据存储的效率\n缺点：编码流程较多\n编码流程 1.在爬虫文件中进行数据解析\ndef parse(self, response): # 如何获取响应数据 # 调用xpath方法对响应数据进行xpath形式的数据解析 li_list = response.xpath('//*[@id=\"list\"]/ul/li') all_data = [] # 爬取到的数据全部都存储到了该列表中 for li in li_list: title = li.xpath('./div[2]/a/text()').extract_first() content = li.xpath('./div[1]/text()').extract_first() 2.将解析到的数据封装到Item类型的对象中\n2.1 在items.py文件中定义相关的字段\nclass SavedataproItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() #爬取的字段有哪些，这里就需要定义哪些变量存储爬取到的字段 title = scrapy.Field() content = scrapy.Field() 2.2 在爬虫文件中引入Item类，实例化item对象，将解析到的数据存储到item对象中\ndef parse(self, response): from items import SavedataproItem #导入item类 # 如何获取响应数据 # 调用xpath方法对响应数据进行xpath形式的数据解析 li_list = response.xpath('//*[@id=\"list\"]/ul/li') all_data = [] # 爬取到的数据全部都存储到了该列表中 for li in li_list: title = li.xpath('./div[2]/a/text()').extract_first() content = li.xpath('./div[1]/text()').extract_first() #实例化一个item类型的对象 item = SavedataproItem() #通过中括号的方式访问item对象中的两个成员，且将解析到的两个字段赋值给item对象的两个成员即可 item['title'] = title item['content'] = content 3.将item对象提交给管道\n#将存储好数据的item对象提交给管道 yield item 4.在管道中接收item类型对象(pipelines.py就是管道文件)\n管道只可以接收item类型的对象，不可以接收其他类型对象\nclass SavedataproPipeline: #process_item用来接收爬虫文件传递过来的item对象 #item参数，就是管道接收到的item类型对象 def process_item(self, item, spider): print(item) return item 5.在管道中对接收到的数据进行任意形式的持久化存储操作\n可以存储到文件中也可以存储到数据库中\n# Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface from itemadapter import ItemAdapter class SavedataproPipeline: #重写父类的方法 fp = None def open_spider(self,spider): print('我是open_spider方法，我在项目开始运行环节，只会被执行一次！') self.fp = open('duanzi.txt','w',encoding='utf-8') #process_item用来接收爬虫文件传递过来的item对象 #item参数，就是管道接收到的item类型对象 #process_item方法调用的次数取决于爬虫文件给其提交item的次数 def process_item(self, item, spider): #item类型的对象其实就是一个字典 # print(item) #将item字典中的标题和内容获取 title = item['title'] content = item['content'] self.fp.write(title+':'+content+'\\n') print(title,':爬取保存成功！') return item def close_spider(self,spider): print('在爬虫结束的时候会被执行一次！') self.fp.close() 6.在配置文件中开启管道机制\n注意：默认情况下，管道机制是没有被开启的，需要在配置文件中手动开启 在setting.py中把ITEM_PIPELINES解除注释就表示开启了管道机制 管道深入操作 如何将数据存储到数据库\n注意：一个管道类负责将数据存储到一个具体的载体中。如果想要将爬取到的数据存储到多个不同的载体/数据库中，则需要定义多个管道类。 思考：\n在有多个管道类的前提下，爬虫文件提交的item会同时给没一个管道类还是单独的管道类？ 爬虫文件只会将item提交给优先级最高的那一个管道类。优先级最高的管道类的process_item中需要写return item操作，该操作就是表示将item对象传递给下一个管道类，下一个管道类获取了item对象，才可以将数据存储成功！ 管道类：\n# Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface from itemadapter import ItemAdapter import pymysql import redis import pymongo #负责将数据存储到mysql中 class MysqlPipeline: conn = None #mysql的链接对象 cursor = None def open_spider(self,spider): self.conn = pymysql.Connect( host = '127.0.0.1', port = 3306, user = 'root', password = 'boboadmin', db = 'spider3qi', charset = 'utf8' ) self.cursor = self.conn.cursor() #爬虫文件每向管道提交一个item，则process_item方法就会被调用一次 def process_item(self, item, spider): title = item['title'] sql = 'insert into xiaoshuo (title) values (\"%s\")'%title self.cursor.execute(sql) self.conn.commit() print('成功写入一条数据！') return item def close_spider(self,spider): self.cursor.close() self.conn.close() #将数据持久化存储到redis中 class RedisPipeLine: conn = None def open_spider(self,spider): #在链接前务必手动启动redis的服务 self.conn = redis.Redis( host='127.0.0.1', port=6379 ) def process_item(self,item,spider): #注意：如果想要将一个python字典直接写入到redis中，则redis模块的版本务必是2.10.6 #如果redis模块的版本不是2.10.6则重新安装：pip install redis==2.10.6 self.conn.lpush('xiaoshuo',item) print('数据存储redis成功！') return item class MongoPipeline: conn = None #链接对象 db_sanqi = None #数据仓库 def open_spider(self,spider): self.conn = pymongo.MongoClient( host='127.0.0.1', port=27017 ) self.db_sanqi = self.conn['sanqi'] def process_item(self,item,spider): self.db_sanqi['xiaoshuo'].insert_one({'title':item['title']}) print('插入成功！') return item 配置文件：\nITEM_PIPELINES = { #数字表示管道类被执行的优先级，数字越小表示优先级越高 'xiaoshuoPro.pipelines.MysqlPipeline': 300, 'xiaoshuoPro.pipelines.RedisPipeLine': 301, 'xiaoshuoPro.pipelines.MongoPipeline': 302, } scrapy爬取多媒体资源数据 使用一个专有的管道类ImagesPipeline\n具体的编码流程：\n1.在爬虫文件中进行图片/视频的链接提取\n2.将提取到的链接封装到items对象中，提交给管道\n3.在管道文件中自定义一个父类为ImagesPipeline的管道类，且重写三个方法即可：\ndef get_media_requests(self, item, info):接收爬虫文件提交过来的item对象，然后对图片地址发起网路请求，返回图片的二进制数据 def file_path(self, request, response=None, info=None, *, item=None)：指定保存图片的名称 def item_completed(self, results, item, info)：返回item对象给下一个管道类 4.在配置文件中开启指定的管道，且通过IMAGES_STORE = ‘girlsLib’操作指定图片存储的文件夹。\n# Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface import scrapy from itemadapter import ItemAdapter from scrapy.pipelines.images import ImagesPipeline #自定义的管道类一定要继承与ImagesPipeline class mediaPileline(ImagesPipeline): #重写三个父类的方法来完成图片二进制数据的请求和持久化存储 #可以根据图片地址，对其进行请求，获取图片数据 #参数item：就是接收到的item对象 def get_media_requests(self, item, info): img_src = item['src'] yield scrapy.Request(img_src) #指定图片的名称（只需要返回图片存储的名称即可） def file_path(self, request, response=None, info=None, *, item=None): imgName = request.url.split('/')[-1] print(imgName,'下载保存成功！') return imgName #如果没有下一个管道类，该方法可以不写 def item_completed(self, results, item, info): return item #可以将当前的管道类接收到item对象传递给下一个管道类2. scrapy深度爬取 如何爬取多页的数据（全站数据爬取）\n手动请求发送：\n#callback用来指定解析方法 yield scrapy.Request(url=new_url,callback=self.parse) 如何爬取深度存储的数据\n什么是深度，说白了就是爬取的数据没有存在于同一张页面中。\n必须使用请求传参的机制才可以完整的实现。\n请求传参：\nyield scrapy.Request(meta={},url=detail_url,callback=self.parse_detail) 可以将meta字典传递给callback这个回调函数 import scrapy from ..items import DeepproItem class DeepSpider(scrapy.Spider): name = 'deep' # allowed_domains = ['www.xxx.com'] start_urls = ['https://wz.sun0769.com/political/index/politicsNewest'] #解析首页数据 def parse(self, response): li_list = response.xpath('/html/body/div[2]/div[3]/ul[2]/li') for li in li_list: title = li.xpath('./span[3]/a/text()').extract_first() detail_url = 'https://wz.sun0769.com'+li.xpath('./span[3]/a/@href').extract_first() # print(title) item = DeepproItem() item['title'] = title #对详情页的url发起请求 #参数meta可以将自身这个字典传递给callback指定的回调函数 yield scrapy.Request(meta={'item':item},url=detail_url,callback=self.parse_detail) #解析详情页数据 def parse_detail(self,response): meta = response.meta #接收请求传参过来的meta字典 item = meta['item'] content = response.xpath('/html/body/div[3]/div[2]/div[2]/div[2]//text()').extract() content = ''.join(content) # print(content) item['content'] = content yield item ImagePipeLines的请求传参 环境安装：pip install Pillow\nUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36' 需求：将图片的名称和详情页中图片的数据进行爬取，持久化存储。\n分析：\n深度爬取：请求传参 多页的数据爬取：手动请求的发送 爬虫文件：\nimport scrapy from ..items import DeepimgproItem class ImgSpider(scrapy.Spider): name = 'img' # allowed_domains = ['www.xxx.com'] start_urls = ['https://pic.netbian.com/4kmeinv/'] #通用的url模板 url_model = 'https://pic.netbian.com/4kmeinv/index_%d.html' page_num = 2 def parse(self, response): #解析出了图片的名称和详情页的url li_list = response.xpath('//*[@id=\"main\"]/div[3]/ul/li') for li in li_list: title = li.xpath('./a/b/text()').extract_first() + '.jpg' detail_url = 'https://pic.netbian.com'+li.xpath('./a/@href').extract_first() item = DeepimgproItem() item['title'] = title #需要对详情页的url发起请求，在详情页中获取图片的下载链接 yield scrapy.Request(url=detail_url,callback=self.detail_parse,meta={'item':item}) if self.page_num \u003c= 2: new_url = format(self.url_model%self.page_num) self.page_num += 1 yield scrapy.Request(url=new_url,callback=self.parse) #解析详情页的数据 def detail_parse(self,response): meta = response.meta item = meta['item'] img_src = 'https://pic.netbian.com'+response.xpath('//*[@id=\"img\"]/img/@src').extract_first() item['img_src'] = img_src yield item 管道：\n# Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface import scrapy from itemadapter import ItemAdapter from scrapy.pipelines.images import ImagesPipeline class DeepimgproPipeline(ImagesPipeline): # def process_item(self, item, spider): # return item def get_media_requests(self, item, info): img_src = item['img_src'] #请求传参，将item中的图片名称传递给file_path方法 #meta会将自身传递给file_path print(item['title'],'保存下载成功！') yield scrapy.Request(url=img_src,meta={'title':item['title']}) def file_path(self, request, response=None, info=None, *, item=None): #返回图片的名称 #接收请求传参过来的数据 title = request.meta['title'] return title def item_completed(self, results, item, info): return item 如何提高scrapy的爬取效率 增加并发： 默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。 降低日志级别： 在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为WORNING或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘ERROR’ 禁止cookie： 如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False 禁止重试： 对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False 减少下载超时： 如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s post请求发送 问题：在之前代码中，我们从来没有手动的对start_urls列表中存储的起始url进行过请求的发送，但是起始url的确是进行了请求的发送，那这是如何实现的呢？\n解答：其实是因为爬虫文件中的爬虫类继承到了Spider父类中的start_requests（self）这个方法，该方法就可以对start_urls列表中的url发起请求：\ndef start_requests(self): for u in self.start_urls: yield scrapy.Request(url=u,callback=self.parse) 【注意】该方法默认的实现，是对起始的url发起get请求，如果想发起post请求，则需要子类重写该方法。\nyield scrapy.Request():发起get请求 yield scrapy.FormRequest():发起post请求 import scrapy class FanyiSpider(scrapy.Spider): name = 'fanyi' # allowed_domains = ['www.xxx.com'] start_urls = ['https://fanyi.baidu.com/sug'] #父类中的方法：该方法是用来给起始的url列表中的每一个url发请求 def start_requests(self): data = { 'kw':'dog' } for url in self.start_urls: #formdata是用来指定请求参数 yield scrapy.FormRequest(url=url,callback=self.parse,formdata=data) def parse(self, response): result = response.json() print(result) scrapy的核心组件 从中可以大致了解scrapy框架的一个运行机制 - 引擎(Scrapy) 用来处理整个系统的数据流处理, 触发事务(框架核心) - 调度器(Scheduler) 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址 - 下载器(Downloader) 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的) - 爬虫(Spiders) 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面 - 项目管道(Pipeline) 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 中间件 scrapy的中间件有两个：\n爬虫中间件 下载中间件 中间件的作用是什么？ 观测中间件在五大核心组件的什么位置，根据位置了解中间件的作用 下载中间件位于引擎和下载器之间 引擎会给下载器传递请求对象，下载器会给引擎返回响应对象。 作用：可以拦截到scrapy框架中所有的请求和响应。 拦截请求干什么？ 修改请求的ip，修改请求的头信息，设置请求的cookie 拦截响应干什么？ 可以修改响应数据 中间件重要方法：\n# Define here the models for your spider middleware # # See documentation in: # https://docs.scrapy.org/en/latest/topics/spider-middleware.html from scrapy import signals # useful for handling different item types with a single interface from itemadapter import is_item, ItemAdapter class MiddleproDownloaderMiddleware: #类方法：作用是返回一个下载器对象（忽略） @classmethod def from_crawler(cls, crawler): # This method is used by Scrapy to create your spiders. s = cls() crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) return s #拦截处理所有的请求对象 #参数：request就是拦截到的请求对象，spider爬虫文件中爬虫类实例化的对象 #spider参数的作用可以实现爬虫类和中间类的数据交互 def process_request(self, request, spider): return None #拦截处理所有的响应对象 #参数：response就是拦截到的响应对象，request就是被拦截到响应对象对应的唯一的一个请求对象 def process_response(self, request, response, spider): return response #拦截和处理发生异常的请求对象 #参数：reqeust就是拦截到的发生异常的请求对象 def process_exception(self, request, exception, spider): pass #控制日志数据的（忽略） def spider_opened(self, spider): spider.logger.info('Spider opened: %s' % spider.name) 开发代理中间件 request.meta[‘proxy’] = proxy\n# Define here the models for your spider middleware # # See documentation in: # https://docs.scrapy.org/en/latest/topics/spider-middleware.html from scrapy import signals # useful for handling different item types with a single interface from itemadapter import is_item, ItemAdapter from scrapy import Request class MiddleproDownloaderMiddleware: #类方法：作用是返回一个下载器对象（忽略） @classmethod def from_crawler(cls, crawler): # This method is used by Scrapy to create your spiders. s = cls() crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) return s #拦截处理所有的请求对象 #参数：request就是拦截到的请求对象，spider爬虫文件中爬虫类实例化的对象 #spider参数的作用可以实现爬虫类和中间类的数据交互 def process_request(self, request, spider): #是的所有的请求都是用代理，则代理操作可以写在该方法中 request.meta['proxy'] = 'http://ip:port' #弊端：会使得整体的请求效率变低 print(request.url+':请求对象拦截成功！') return None #拦截处理所有的响应对象 #参数：response就是拦截到的响应对象，request就是被拦截到响应对象对应的唯一的一个请求对象 def process_response(self, request, response, spider): print(request.url+':响应对象拦截成功！') return response #拦截和处理发生异常的请求对象 #参数：reqeust就是拦截到的发生异常的请求对象 #方法存在的意义：将发生异常的请求拦截到，然后对其进行修正 def process_exception(self, request, exception, spider): print(request.url+':发生异常的请求对象被拦截到！') #修正操作 #只有发生了异常的请求才使用代理机制，则可以写在该方法中 request.meta['proxy'] = 'https://ip:port' return request #对请求对象进行重新发送 #控制日志数据的（忽略） def spider_opened(self, spider): spider.logger.info('Spider opened: %s' % spider.name) 开发UA中间件 request.headers[‘User-Agent’] = ua\ndef process_request(self, request, spider): request.headers['User-Agent'] = '从列表中随机选择的一个UA值' print(request.url+':请求对象拦截成功！') return None 开发Cookie中间件 request.cookies = cookies\ndef process_request(self, request, spider): request.headers['cookie'] = 'xxx' #request.cookies = 'xxx' print(request.url+':请求对象拦截成功！') return None selenium+scrapy 需求：将网易新闻中的国内，国际，军事，航空四个板块下的新闻标题和内容进行数据爬取\n注意：哪些数据是动态加载的！ 技术：selenium，scrapy，中间件 分析：\n抓取首页中四个板块下所有的新闻标题和新闻内容 获取首页中四个板块对应的详情页链接 首页是没有动态加载数据，可以直接爬取+解析 对每一个板块的url发起请求，获取详情页中的新闻标题等内容 通过分析发现每一个板块中的新闻数据全部是动态加载的数据，如何解决呢？ 通过selenium解决 scrapy+selenium的编码流程\n1.在爬虫文件中定义浏览器对象，将浏览器对象作为爬虫类的一个成员变量 2.在中间件中通过spider获取爬虫文件中定义的浏览器对象，进行请求发送和获取响应数据 3.在爬虫文件中重写一个closed方法，来关闭浏览器对象 爬虫文件\nimport scrapy from ..items import WangyiproItem from selenium import webdriver class WangyiSpider(scrapy.Spider): name = 'wangyi' # allowed_domains = ['www.xxx.com'] start_urls = ['https://news.163.com/'] #创建浏览器对象，把浏览器对象作为爬虫类的一个成员 bro = webdriver.Chrome(executable_path='/Users/zhangxiaobo/Desktop/三期/chromedriver1') model_urls = [] #存储4个板块对应的url def parse(self, response): #从首页解析每一个板块对应详情页的url，将其存储到model_urls列表中 model_index = [2,3,5,6] li_list = response.xpath('//*[@id=\"index2016_wrap\"]/div[3]/div[2]/div[2]/div[2]/div/ul/li') for index in model_index: model_url = li_list[index].xpath('./a/@href').extract_first() self.model_urls.append(model_url) #应该对每一个板块的详情页发起请求（动态加载） for model_url in self.model_urls: yield scrapy.Request(url=model_url,callback=self.parse_detail) #目的是为了解析出每一个板块中的新闻标题和新闻详情页的url def parse_detail(self,response): #response就是一个不符合需求要求的响应对象 #该response中没有存储动态加载的新闻数据，因此该响应对象被视为不符合要求的响应对象 #需要将不符合要求的响应对象变为符合要求的响应对象即可，如何做呢？ #方法：篡改不符合要求的响应对象的响应数据，将该响应对象的响应数据修改为包含了动态加载的新闻数据即可。 div_list = response.xpath('/html/body/div/div[3]/div[4]/div[1]/div[1]/div/ul/li/div/div') for div in div_list: try: #解析新闻标题+新闻详情页的url title = div.xpath('./div/div[1]/h3/a/text()').extract_first() new_detail_url = div.xpath('./div/div[1]/h3/a/@href').extract_first() item = WangyiproItem() item['title'] = title except Exception as e: print('遇到了广告，忽略此次行为即可！') #对新闻的详情页发起请求 if new_detail_url != None: yield scrapy.Request(url=new_detail_url,callback=self.new_content_parse,meta={'item':item}) def new_content_parse(self,response): item = response.meta['item'] #解析新闻的详情内容 content = response.xpath('//*[@id=\"content\"]/div[2]//text()').extract() content = ''.join(content).strip() item['content'] = content yield item #重写一个父类方法，close_spider，该方法只会在爬虫最后执行一次 def closed(self,spider): #关闭浏览器 print('关闭浏览器成功！') self.bro.quit() 中间件文件：\n# Define here the models for your spider middleware # # See documentation in: # https://docs.scrapy.org/en/latest/topics/spider-middleware.html import requests from scrapy import signals # useful for handling different item types with a single interface from itemadapter import is_item, ItemAdapter from time import sleep from scrapy.http import HtmlResponse#scrapy封装的响应对象对应的类 class WangyiproDownloaderMiddleware: def process_request(self, request, spider): return None def process_response(self, request, response, spider): #可以拦截到所有的响应对象 #当前项目一共会产生多少个响应对象呢？ #1 + 4 + n个响应对象，在这些响应对象中只有4这4个响应对象需要被修改 #如何筛选出指定的4个板块对应的响应对象呢？ #1.可以先找出指定4个板块的请求对象，然后根据请求对象定位指定4个响应对象 #2.可以根据4个板块的url定位到四个板块的请求对象 model_urls = spider.model_urls if request.url in model_urls: bro = spider.bro #从爬虫类中获取创建好的浏览器对象 bro.get(request.url) sleep(1) # bro.execute_script('document.documentElement.scrollTo(0,9000)') # sleep(1) #获取动态加载的数据 page_text = bro.page_source #说明该request就是指定响应对象的请求对象 #此处的response就是指定板块对应的响应对象 response = HtmlResponse(url=request.url, request=request, encoding='utf-8', body=page_text) #body就是响应对象的响应数据 return response else: return response def process_exception(self, request, exception, spider): pass 配置文件：\nDOWNLOADER_MIDDLEWARES = { 'wangyiPro.middlewares.WangyiproDownloaderMiddleware': 543, } 拓展功能：将人工智能+数据爬取中\n实现将爬取到的新闻进行分类和关键字提取\n百度AI的使用：https://ai.baidu.com/\n使用流程：\n点击首页右上角的控制台，进行登录。\n登录后进入到了智能云的首页\n点击页面左上角的三条杠，选择你想要实现的功能，点击，进入到指定功能页面\n在功能页面，首先点击【创建应用】，进行应用的创建 创建好之后，点击管理应用就可以看到： AppID，apiKey，secret key这三个值，会在程序中用到 在功能页面点击左侧的【技术文档】，选择SDK说明，选择对应的Python语言即可，先看快速开始内容，在选择你想要实现的具体功能的文档界面即可。\n环境安装：pip install baidu-aip 提取文章关键字：\nfrom aip import AipNlp \"\"\" 你的 APPID AK SK \"\"\" APP_ID = 'xxx' API_KEY = 'xxx' SECRET_KEY = 'xxx' client = AipNlp(APP_ID, API_KEY, SECRET_KEY) title = \"iphone手机出现“白苹果”原因及解决办法，用苹果手机的可以看下\" content = \"如果下面的方法还是没有解决你的问题建议来我们门店看下成都市锦江区红星路三段99号银石广场24层01室。\" \"\"\" 调用文章标签 \"\"\" result = client.keyword(title, content) for dic in result['items']: if dic['score'] \u003e= 0.8: key = dic['tag'] print(key) 文章分类：\nfrom aip import AipNlp \"\"\" 你的 APPID AK SK \"\"\" APP_ID = 'xxx' API_KEY = 'x' SECRET_KEY = 'xx' client = AipNlp(APP_ID, API_KEY, SECRET_KEY) title = \"秦刚访问特斯拉美国工厂马斯克陪同 传递什么信号？\" content = \"今天（4日），驻美大使秦刚访问了特斯拉硅谷工厂，同特斯拉CEO马斯克针对各项尖端科技、人类未来等主题展开探讨，并体验了特斯拉的新款Model S及最新自动辅助驾驶系统。他在海外社交平台上表示：“性能强劲，但乘坐平顺舒适”。\" \"\"\" 调用文章分类 \"\"\" result = client.topic(title, content) class_new = result['item']['lv1_tag_list'][0]['tag'] print(class_new) CrawlSpider 实现网站的全站数据爬取\n就是将网站中所有页码对应的页面数据进行爬取。 crawlspider其实就是scrapy封装好的一个爬虫类，通过该类提供的相关的方法和属性就可以实现全新高效形式的全站数据爬取。\n使用流程：\n新建一个scrapy项目\ncd 项目\n创建爬虫文件（*）：\nscrapy genspider-t crawl spiderName www.xxx.com\n爬虫文件中发生的变化有哪些？\n当前爬虫类的父类为CrawlSpider 爬虫类中多了一个类变量叫做rules LinkExtractor：链接提取器 可以根据allow参数表示的正则在当前页面中提取符合正则要求的链接 Rule：规则解析器 可以接收链接提取器提取到的链接，并且对每一个链接进行请求发送 可以根据callback指定的回调函数对每一次请求到的数据进行数据解析 思考:如何将一个网站中所有的链接都提取到呢？ 只需要在链接提取器的allow后面赋值一个空正则表达式即可 目前在scrapy中有几种发送请求的方式？ start_urls列表可以发送请求 scrapy.Request() scrapy.FormRequest() Rule规则解析器 注意：\n链接提取器和规则解析器是一一对应的（一对一的关系） 建议在使用crawlSpider实现深度爬取的时候，需要配合手动请求发送的方式进行搭配！ USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36' 分布式 分布式在日常开发中并不常用，只是一个噱头！\n概念：\n可以使用多台电脑搭建一个分布式机群，使得多台对电脑可以对同一个网站的数据进行联合且分布的数据爬取。 声明：\n原生的scrapy框架并无法实现分布式操作！why？ 多台电脑之间无法共享同一个调度器 多台电脑之间无法共享同一个管道 如何是的scrapy可以实现分布式呢？\n借助于一个组件：scrapy-redis scrapy-redis的作用是什么？ 可以给原生的scrapy框架提供可被共享的调度器和管道！ 环境安装：pip install scrapy-redis 注意：scrapy-redis该组件只可以将爬取到的数据存储到redis数据库 编码流程（重点）：\n1.创建项目\n2.cd 项目\n3.创建基于crawlSpider的爬虫文件\n3.1 修改爬虫文件 导包：from scrapy_redis.spiders import RedisCrawlSpider 修改当前爬虫类的父类为 RedisCrawlSpider 将start_urls替换成redis_key的操作 redis_key变量的赋值为字符串，该字符串表示调度器队列的名称 进行常规的请求操作和数据解析 4.settings配置文件的修改\n常规内容修改（robots和ua等），先不指定日志等级\n指定可以被共享的管道类\nITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 400 } 指定可以被共享的调度器\n# 使用scrapy-redis组件的去重队列 DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" # 使用scrapy-redis组件自己的调度器 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" # 是否允许暂停 SCHEDULER_PERSIST = True 指定数据库\nREDIS_HOST = '127.0.0.1' REDIS_PORT = 6379 5.修改redis数据库的配置文件（redis.windows.conf）\n在配置文件中改行代码是没有没注释的：\nbind 127.0.0.1 #将上述代码注释即可（解除本机绑定，实现外部设备访问本机数据库 如果配置文件中还存在：protected-mode = true，将true修改为false， 修改为false后表示redis数据库关闭了保护模式，表示其他设备可以远程访问且修改你数据库中的数据 6.启动redis数据库的服务端和客户端\n7.运行项目,发现程序暂定一直在等待，等待爬取任务\n8.需要向可以被共享的调度器的队列（redis_key的值）中放入一个起始的url\n在redis数据库的客户端执行如下操作：\nlpush 队列名称 起始的url 起始url：https://wz.sun0769.com/political/index/politicsNewest?id=1\u0026page=1 增量式 爬虫应用场景分类\n通用爬虫 聚焦爬虫 功能爬虫 分布式爬虫 增量式： 用来监测网站数据更新的情况（爬取网站最新更新出来的数据）。 只是一种程序设计的思路，使用什么技术都是可以实现的。 核心： 去重。 使用一个记录表来实现数据的去重： 记录表：存储爬取过的数据的记录 如何构建和设计一个记录表： 记录表需要具备的特性： 去重 需要持久保存的 方案1：使用Python的set集合充当记录表？ 不可以的！因为set集合无法实现持久化存储 方案2：使用redis的set集合充当记录表？ 可以的，因为redis的set既可以实现去重又可以进行数据的持久化存储。 基于两个场景实现增量式爬虫：\n场景1：如果爬取的数据都是存储在当前网页中，没有深度的数据爬取的必要。 场景2：爬取的数据存在于当前页和详情页中，具备深度爬取的必要。 场景1的实现：\n数据指纹：\n数据的唯一标识。记录表中可以不直接存储数据本身，直接存储数据指纹更好一些。\n#爬虫文件 import scrapy import redis from ..items import Zlsdemo1ProItem class DuanziSpider(scrapy.Spider): name = 'duanzi' # allowed_domains = ['www.xxxx.com'] start_urls = ['https://ishuo.cn/'] #Redis的链接对象 conn = redis.Redis(host='127.0.0.1',port=6379) def parse(self, response): li_list = response.xpath('//*[@id=\"list\"]/ul/li') for li in li_list: content = li.xpath('./div[1]/text()').extract_first() title = li.xpath('./div[2]/a/text()').extract_first() all_data = title+content #生成该数据的数据指纹 import hashlib # 导入一个生成数据指纹的模块 m = hashlib.md5() m.update(all_data.encode('utf-8')) data_id = m.hexdigest() ex = self.conn.sadd('data_id',data_id) if ex == 1:#sadd执行成功（数据指纹在set集合中不存在） print('有最新数据的更新，正在爬取中......') item = Zlsdemo1ProItem() item['title'] = title item['content'] = content yield item else:#sadd没有执行成功（数据指纹在set集合中存储） print('暂无最新数据更新，请等待......') 场景2的实现：\n使用详情页的url充当数据指纹即可。\nimport scrapy import redis from ..items import Zlsdemo2ProItem class JianliSpider(scrapy.Spider): name = 'jianli' # allowed_domains = ['www.xxx.com'] start_urls = ['https://sc.chinaz.com/jianli/free.html'] conn = redis.Redis(host='127.0.0.1',port=6379) def parse(self, response): div_list = response.xpath('//*[@id=\"container\"]/div') for div in div_list: title = div.xpath('./p/a/text()').extract_first() #充当数据指纹 detail_url = 'https:'+div.xpath('./p/a/@href').extract_first() ex = self.conn.sadd('data_id',detail_url) item = Zlsdemo2ProItem() item['title'] = title if ex == 1: print('有最新数据的更新，正在采集......') yield scrapy.Request(url=detail_url,callback=self.parse_detail,meta={'item':item}) else: print('暂无数据更新！') def parse_detail(self,response): item = response.meta['item'] download_url = response.xpath('//*[@id=\"down\"]/div[2]/ul/li[1]/a/@href').extract_first() item['download_url'] = download_url yield item scrapy项目部署 scrapyd部署工具介绍 scrapyd是一个用于部署和运行scrapy爬虫的程序，它由 scrapy 官方提供的。它允许你通过JSON API来部署爬虫项目和控制爬虫运行。 所谓json api本质就是post请求的webapi\n选择一台主机当做服务器，安装并启动 scrapyd 服务。再这之后，scrapyd 会以守护进程的方式存在系统中，监听爬虫地运行与请求，然后启动进程来执行爬虫程序。 环境安装 scrapyd服务: ​\tpip install scrapyd\nscrapyd客户端: ​\tpip install scrapyd-client\n​\t一定要安装较新的版本10以上的版本，如果是现在安装的一般都是新版本\n启动scrapyd服务 打开终端在scrapy项目路径下 启动scrapyd的命令： scrapyd scrapyd 也提供了 web 的接口。方便我们查看和管理爬虫程序。默认情况下 scrapyd 监听 6800 端口，运行 scrapyd 后。在本机上使用浏览器访问 http://localhost:6800/地址即可查看到当前可以运行的项目。 点击job可以查看任务监控界面 scrapy项目部署 配置需要部署的项目 编辑需要部署的项目的scrapy.cfg文件(需要将哪一个爬虫部署到scrapyd中，就配置该项目的该文件) [deploy:部署名(部署名可以自行定义)] url = http://localhost:6800/ project = 项目名(创建爬虫项目时使用的名称) username = bobo # 如果不需要用户名可以不写 password = 123456 # 如果不需要密码可以不写 部署项目到scrapyd 同样在scrapy项目路径下执行如下指令：\nscrapyd-deploy 部署名(配置文件中设置的名称) -p 项目名称 部署成功之后就可以看到部署的项目\n使用以下命令检查部署爬虫结果：\nscrapyd-deploy -L 部署名 管理scrapy项目 指令管理 安装curl命令行工具\nwindow需要安装 linux和mac无需单独安装 window安装步骤：\n下载curl文件：https://curl.se/download.html，打开网页后向下拖动，找到window系统对应版本下载 下载后，放置到一个无中文的文件夹下直接解压缩，解压后将bin文件夹配置环境变量！ 参考网页：https://www.cnblogs.com/lisa2016/p/12193494.html 启动项目：\ncurl http://localhost:6800/schedule.json -d project=项目名 -d spider=爬虫名 返回结果：注意期中的jobid，在关闭项目时候会用到\n{\"status\": \"ok\", \"jobid\": \"94bd8ce041fd11e6af1a000c2969bafd\", \"node_name\": \"james-virtual-machine\"} 关闭项目：\ncurl http://localhost:6800/cancel.json -d project=项目名 -d job=项目的jobid 删除爬虫项目：\ncurl http://localhost:6800/delproject.json -d project=爬虫项目名称 requests模块控制scrapy项目 import requests # 启动爬虫 url = 'http://localhost:6800/schedule.json' data = { 'project': 项目名, 'spider': 爬虫名, } resp = requests.post(url, data=data) # 停止爬虫 url = 'http://localhost:6800/cancel.json' data = { 'project': 项目名, 'job': 启动爬虫时返回的jobid, } resp = requests.post(url, data=data) 生产者消费者模式 认识生产者和消费者模式 生产者和消费者是异步爬虫中很常见的一个问题。产生数据的模块，我们称之为生产者，而处理数据的模块，就称为消费者。\n例如：\n​\t图片数据爬取中，解析出图片链接的操作就是在生产数据\n​\t对图片链接发起请求下载图片的操作就是在消费数据\n为什么要使用生产者和消费者模式 ​ 在异步世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。\nimport requests import threading from lxml import etree from queue import Queue from urllib.request import urlretrieve from time import sleep headers = { \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\", } #生产数据：解析提取图片地址 class Producer(threading.Thread):#生产者线程 def __init__(self,page_queue,img_queue): super().__init__() self.page_queue = page_queue self.img_queue = img_queue def run(self): while True: if self.page_queue.empty(): print('Producer任务结束') break #从page_queue中取出一个页码链接 url = self.page_queue.get() #从当前的页码对应的页面中解析出更多的图片地址 self.parse_detail(url) def parse_detail(self,url): response = requests.get(url,headers=headers) response.encoding = 'gbk' page_text = response.text tree = etree.HTML(page_text) li_list = tree.xpath('//*[@id=\"main\"]/div[3]/ul/li') for li in li_list: img_src = 'https://pic.netbian.com'+li.xpath('./a/img/@src')[0] img_title = li.xpath('./a/b/text()')[0]+'.jpg' dic = { 'title':img_title, 'src':img_src } self.img_queue.put(dic) #消费数据：对图片地址进行数据请求 class Consumer(threading.Thread):#消费者线程 def __init__(self,page_queue,img_queue): super().__init__() self.page_queue = page_queue self.img_queue = img_queue def run(self): while True: if self.img_queue.empty() and self.page_queue.empty(): print('Consumer任务结束') break dic = self.img_queue.get() title = dic['title'] src = dic['src'] print(src) urlretrieve(src,'imgs/'+title) print(title,'下载完毕！') def main(): #该队列中存储即将要要去的页面页码链接 page_queue = Queue(20) #该队列存储生产者生产出来的图片地址 img_queue = Queue(60) #该循环可以将2，3，4这三个页码链接放入page_queue中 for x in range(2,10): url = 'https://pic.netbian.com/4kmeinv/index_%d.html'%x page_queue.put(url) #生产者 for x in range(3): t = Producer(page_queue,img_queue) t.start() #消费者 for x in range(3): t = Consumer(page_queue,img_queue) t.start() main() ","wordCount":"13618","inLanguage":"en","datePublished":"2023-02-10T15:08:28+08:00","dateModified":"2023-02-10T15:08:28+08:00","author":[{"@type":"Person","name":"Felix"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://canw0916.github.io/en/posts/tech/scrapy%E6%A1%86%E6%9E%B6/"},"publisher":{"@type":"Organization","name":"Felix's Blog","logo":{"@type":"ImageObject","url":"https://canw0916.github.io/img/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://canw0916.github.io/en/ accesskey=h title="Felix's Blog (Alt + H)"><img src=https://canw0916.github.io/img/android-chrome-512x512.png alt=logo aria-label=logo height=35>Felix's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://canw0916.github.io/en/search title="🔍 搜索 (Alt + /)" accesskey=/><span>🔍 搜索</span></a></li><li><a href=https://canw0916.github.io/en/ title="🏠 主页"><span>🏠 主页</span></a></li><li><a href=https://canw0916.github.io/en/posts title="📚 文章"><span>📚 文章</span></a></li><li><a href=https://canw0916.github.io/en/tags title="💡 标签"><span>💡 标签</span></a></li><li><a href=https://canw0916.github.io/en/archives/ title="⏱️ 时间轴"><span>⏱️ 时间轴</span></a></li><li><a href=https://canw0916.github.io/en/about title="👦 关于"><span>👦 关于</span></a></li><li><a href=https://canw0916.github.io/en/links title="😺 友链"><span>😺 友链</span></a></li></ul></nav></header><main class="main page"><article class=post-single><div id=single-content><header class=post-header><div class=breadcrumbs><a href=https://canw0916.github.io/en/>🏠主页</a>&nbsp;»&nbsp;<a href=https://canw0916.github.io/en/posts/>📚文章</a>&nbsp;»&nbsp;<a href=https://canw0916.github.io/en/posts/tech/>🚀 技术</a></div><h1 class=post-title>Scrapy框架</h1><div class=post-meta>创建:2023-02-10|字数:13618字|时长: 28分钟|
作者:Felix
&nbsp;&nbsp;标签: &nbsp;
<span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>文章目录</span></summary><div class=inner><ul><li><a href=#%e7%ae%80%e4%bb%8b aria-label=简介>简介</a></li><li><a href=#%e5%ae%89%e8%a3%85 aria-label=安装>安装</a></li><li><a href=#%e5%9f%ba%e6%9c%ac%e4%bd%bf%e7%94%a8 aria-label=基本使用>基本使用</a></li><li><a href=#%e6%95%b0%e6%8d%ae%e8%a7%a3%e6%9e%90 aria-label=数据解析>数据解析</a></li><li><a href=#%e6%8c%81%e4%b9%85%e5%8c%96%e5%ad%98%e5%82%a8 aria-label=持久化存储>持久化存储</a><ul><li><a href=#%e5%9f%ba%e4%ba%8e%e7%bb%88%e7%ab%af%e6%8c%87%e4%bb%a4%e7%9a%84%e6%8c%81%e4%b9%85%e5%8c%96%e5%ad%98%e5%82%a8 aria-label=基于终端指令的持久化存储>基于终端指令的持久化存储</a></li><li><a href=#%e5%9f%ba%e4%ba%8e%e7%ae%a1%e9%81%93%e5%ae%9e%e7%8e%b0%e6%8c%81%e4%b9%85%e5%8c%96%e5%ad%98%e5%82%a8 aria-label=基于管道实现持久化存储>基于管道实现持久化存储</a><ul><li><a href=#%e7%bc%96%e7%a0%81%e6%b5%81%e7%a8%8b aria-label=编码流程>编码流程</a></li></ul></li></ul></li><li><a href=#%e7%ae%a1%e9%81%93%e6%b7%b1%e5%85%a5%e6%93%8d%e4%bd%9c aria-label=管道深入操作>管道深入操作</a></li><li><a href=#scrapy%e7%88%ac%e5%8f%96%e5%a4%9a%e5%aa%92%e4%bd%93%e8%b5%84%e6%ba%90%e6%95%b0%e6%8d%ae aria-label=scrapy爬取多媒体资源数据>scrapy爬取多媒体资源数据</a></li><li><a href=#scrapy%e6%b7%b1%e5%ba%a6%e7%88%ac%e5%8f%96 aria-label=scrapy深度爬取>scrapy深度爬取</a></li><li><a href=#imagepipelines%e7%9a%84%e8%af%b7%e6%b1%82%e4%bc%a0%e5%8f%82 aria-label=ImagePipeLines的请求传参>ImagePipeLines的请求传参</a></li><li><a href=#%e5%a6%82%e4%bd%95%e6%8f%90%e9%ab%98scrapy%e7%9a%84%e7%88%ac%e5%8f%96%e6%95%88%e7%8e%87 aria-label=如何提高scrapy的爬取效率>如何提高scrapy的爬取效率</a></li><li><a href=#post%e8%af%b7%e6%b1%82%e5%8f%91%e9%80%81 aria-label=post请求发送>post请求发送</a></li><li><a href=#scrapy%e7%9a%84%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6 aria-label=scrapy的核心组件>scrapy的核心组件</a></li><li><a href=#%e4%b8%ad%e9%97%b4%e4%bb%b6 aria-label=中间件>中间件</a><ul><li><a href=#%e5%bc%80%e5%8f%91%e4%bb%a3%e7%90%86%e4%b8%ad%e9%97%b4%e4%bb%b6 aria-label=开发代理中间件>开发代理中间件</a></li><li><a href=#%e5%bc%80%e5%8f%91ua%e4%b8%ad%e9%97%b4%e4%bb%b6 aria-label=开发UA中间件>开发UA中间件</a></li><li><a href=#%e5%bc%80%e5%8f%91cookie%e4%b8%ad%e9%97%b4%e4%bb%b6 aria-label=开发Cookie中间件>开发Cookie中间件</a></li><li><a href=#seleniumscrapy aria-label=selenium+scrapy>selenium+scrapy</a></li></ul></li><li><a href=#crawlspider aria-label=CrawlSpider>CrawlSpider</a></li><li><a href=#%e5%88%86%e5%b8%83%e5%bc%8f aria-label=分布式>分布式</a></li><li><a href=#%e5%a2%9e%e9%87%8f%e5%bc%8f aria-label=增量式>增量式</a></li><li><a href=#scrapy%e9%a1%b9%e7%9b%ae%e9%83%a8%e7%bd%b2 aria-label=scrapy项目部署>scrapy项目部署</a><ul><li><a href=#scrapyd%e9%83%a8%e7%bd%b2%e5%b7%a5%e5%85%b7%e4%bb%8b%e7%bb%8d aria-label=scrapyd部署工具介绍>scrapyd部署工具介绍</a></li><li><a href=#%e7%8e%af%e5%a2%83%e5%ae%89%e8%a3%85 aria-label=环境安装>环境安装</a></li><li><a href=#%e5%90%af%e5%8a%a8scrapyd%e6%9c%8d%e5%8a%a1 aria-label=启动scrapyd服务>启动scrapyd服务</a></li><li><a href=#scrapy%e9%a1%b9%e7%9b%ae%e9%83%a8%e7%bd%b2-1 aria-label=scrapy项目部署>scrapy项目部署</a><ul><li><a href=#%e9%85%8d%e7%bd%ae%e9%9c%80%e8%a6%81%e9%83%a8%e7%bd%b2%e7%9a%84%e9%a1%b9%e7%9b%ae aria-label=配置需要部署的项目>配置需要部署的项目</a></li><li><a href=#%e9%83%a8%e7%bd%b2%e9%a1%b9%e7%9b%ae%e5%88%b0scrapyd aria-label=部署项目到scrapyd>部署项目到scrapyd</a></li></ul></li><li><a href=#%e7%ae%a1%e7%90%86scrapy%e9%a1%b9%e7%9b%ae aria-label=管理scrapy项目>管理scrapy项目</a><ul><li><a href=#%e6%8c%87%e4%bb%a4%e7%ae%a1%e7%90%86 aria-label=指令管理>指令管理</a></li><li><a href=#requests%e6%a8%a1%e5%9d%97%e6%8e%a7%e5%88%b6scrapy%e9%a1%b9%e7%9b%ae aria-label=requests模块控制scrapy项目>requests模块控制scrapy项目</a></li></ul></li></ul></li><li><a href=#%e7%94%9f%e4%ba%a7%e8%80%85%e6%b6%88%e8%b4%b9%e8%80%85%e6%a8%a1%e5%bc%8f aria-label=生产者消费者模式>生产者消费者模式</a><ul><li><a href=#%e8%ae%a4%e8%af%86%e7%94%9f%e4%ba%a7%e8%80%85%e5%92%8c%e6%b6%88%e8%b4%b9%e8%80%85%e6%a8%a1%e5%bc%8f aria-label=认识生产者和消费者模式>认识生产者和消费者模式</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e4%bd%bf%e7%94%a8%e7%94%9f%e4%ba%a7%e8%80%85%e5%92%8c%e6%b6%88%e8%b4%b9%e8%80%85%e6%a8%a1%e5%bc%8f aria-label=为什么要使用生产者和消费者模式>为什么要使用生产者和消费者模式</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement||=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e}),elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h3 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h3><p>什么是框架？</p><p>所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。</p><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。</p><p>初期如何学习框架？</p><p>只需要学习框架集成好的各种功能的用法即可！前期切勿钻研框架的源码！</p><h3 id=安装>安装<a hidden class=anchor aria-hidden=true href=#安装>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Linux/mac系统：
</span></span><span class=line><span class=cl>      pip install scrapy（任意目录下）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Windows系统：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      a. pip install wheel（任意目录下）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      b. 下载twisted文件，下载网址如下： http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      c. 终端进入下载目录，执行 pip install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl
</span></span><span class=line><span class=cl>      注意：如果该步骤安装出错，则换一个版本的whl文件即可
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      d. pip install pywin32（任意目录下）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      e. pip install scrapy（任意目录下）
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>如果安装好后，在终端中录入scrapy指令按下回车，如果没有提示找不到该指令，则表示安装成功
</span></span></code></pre></div><h3 id=基本使用>基本使用<a hidden class=anchor aria-hidden=true href=#基本使用>#</a></h3><ul><li><p>创建项目</p><ul><li><p>scrapy startproject 项目名称</p></li><li><p>项目的目录结构：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  firstBlood   # 项目所在文件夹, 建议用pycharm打开该文件夹
</span></span><span class=line><span class=cl>      ├── firstBlood  		# 项目跟目录
</span></span><span class=line><span class=cl>      │   ├── __init__.py
</span></span><span class=line><span class=cl>      │   ├── items.py  		# 封装数据的格式
</span></span><span class=line><span class=cl>      │   ├── middlewares.py  # 所有中间件
</span></span><span class=line><span class=cl>      │   ├── pipelines.py	# 所有的管道
</span></span><span class=line><span class=cl>      │   ├── settings.py		# 爬虫配置信息
</span></span><span class=line><span class=cl>      │   └── spiders			# 爬虫文件夹, 稍后里面会写入爬虫代码
</span></span><span class=line><span class=cl>      │       └── __init__.py
</span></span><span class=line><span class=cl>      └── scrapy.cfg			# scrapy项目配置信息,不要删它,别动它,善待它. 
</span></span></code></pre></div></li></ul></li></ul></li><li><p>创建爬虫爬虫文件：</p><ul><li>cd project_name（进入项目目录）</li><li>scrapy genspider 爬虫文件的名称（自定义一个名字即可） 起始url<ul><li>（例如：scrapy genspider first <a href=https://www.xxx.com>www.xxx.com</a>）</li></ul></li><li>创建成功后，会在爬虫文件夹下生成一个py的爬虫文件</li></ul></li><li><p>编写爬虫文件</p><ul><li><p>理解爬虫文件的不同组成部分</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>FirstSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=c1>#爬虫名称：爬虫文件唯一标识：可以使用该变量的值来定位到唯一的一个爬虫文件</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;first&#39;</span> <span class=c1>#无需改动</span>
</span></span><span class=line><span class=cl>      <span class=c1>#允许的域名：scrapy只可以发起百度域名下的网络请求</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.baidu.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=c1>#起始的url列表：列表中存放的url可以被scrapy发起get请求</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://www.baidu.com/&#39;</span><span class=p>,</span><span class=s1>&#39;https://www.sogou.com&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1>#专门用作于数据解析</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数response：就是请求之后对应的响应对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#parse的调用次数，取决于start_urls列表元素的个数</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;响应对象为：&#39;</span><span class=p>,</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div></li></ul></li><li><p>配置文件修改:settings.py</p><ul><li>不遵从robots协议：ROBOTSTXT_OBEY = False</li><li>指定输出日志的类型：LOG_LEVEL = &lsquo;ERROR&rsquo;</li><li>指定UA：USER_AGENT = &lsquo;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36&rsquo;</li></ul></li><li><p>运行项目</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  scrapy crawl 爬虫名称 ：该种执行形式会显示执行的日志信息（推荐）
</span></span><span class=line><span class=cl>  scrapy crawl 爬虫名称 --nolog：该种执行形式不会显示执行的日志信息（一般不用）
</span></span></code></pre></div></li></ul></li></ul><h3 id=数据解析>数据解析<a hidden class=anchor aria-hidden=true href=#数据解析>#</a></h3><ul><li><p>注意，如果终端还在第一个项目的文件夹中，则需要在终端中执行cd ../返回到上级目录，在去新建另一个项目。</p></li><li><p>新建数据解析项目：</p><ul><li>创建工程：scrapy startproject 项目名称</li><li>cd 项目名称</li><li>创建爬虫文件：scrapy genspider 爬虫文件名 <a href=https://www.xxx.com>www.xxx.com</a></li></ul></li><li><p>配置文件的修改：settings.py</p><ul><li>不遵从robots协议：ROBOTSTXT_OBEY = False</li><li>指定输出日志的类型：LOG_LEVEL = &lsquo;ERROR&rsquo;</li><li>指定UA：USER_AGENT = &lsquo;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36&rsquo;</li></ul></li><li><p>编写爬虫文件：spiders/duanzi.py</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>DuanziSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;duanzi&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=c1>#对首页进行网络请求</span>
</span></span><span class=line><span class=cl>      <span class=c1>#scrapy会对列表中的url发起get请求</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://ishuo.cn/duanzi&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#如何获取响应数据</span>
</span></span><span class=line><span class=cl>          <span class=c1>#调用xpath方法对响应数据进行xpath形式的数据解析</span>
</span></span><span class=line><span class=cl>          <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;list&#34;]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=c1># content = li.xpath(&#39;./div[1]/text()&#39;)[0]</span>
</span></span><span class=line><span class=cl>              <span class=c1># title = li.xpath(&#39;./div[2]/a/text()&#39;)[0]</span>
</span></span><span class=line><span class=cl>              <span class=c1># #&lt;Selector xpath=&#39;./div[2]/a/text()&#39; data=&#39;一年奔波，尘缘遇了谁&#39;&gt;</span>
</span></span><span class=line><span class=cl>              <span class=c1># print(title)#selector的对象，且我们想要的字符串内容存在于该对象的data参数里</span>
</span></span><span class=line><span class=cl>              <span class=c1>#解析方案1：</span>
</span></span><span class=line><span class=cl>              <span class=c1># title = li.xpath(&#39;./div[2]/a/text()&#39;)[0]</span>
</span></span><span class=line><span class=cl>              <span class=c1># content = li.xpath(&#39;./div[1]/text()&#39;)[0]</span>
</span></span><span class=line><span class=cl>              <span class=c1># #extract()可以将selector对象中data参数的值取出</span>
</span></span><span class=line><span class=cl>              <span class=c1># print(title.extract())</span>
</span></span><span class=line><span class=cl>              <span class=c1># print(content.extract())</span>
</span></span><span class=line><span class=cl>              <span class=c1>#解析方案2：</span>
</span></span><span class=line><span class=cl>              <span class=c1>#title和content为列表，列表只要一个列表元素</span>
</span></span><span class=line><span class=cl>              <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[2]/a/text()&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=n>content</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[1]/text()&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=c1>#extract_first()可以将列表中第0个列表元素表示的selector对象中data的参数值取出</span>
</span></span><span class=line><span class=cl>              <span class=nb>print</span><span class=p>(</span><span class=n>title</span><span class=o>.</span><span class=n>extract_first</span><span class=p>())</span>
</span></span><span class=line><span class=cl>              <span class=nb>print</span><span class=p>(</span><span class=n>content</span><span class=o>.</span><span class=n>extract_first</span><span class=p>())</span>
</span></span></code></pre></div></li></ul></li></ul><h3 id=持久化存储>持久化存储<a hidden class=anchor aria-hidden=true href=#持久化存储>#</a></h3><p>两种方案：</p><ul><li>基于终端指令的持久化存储</li><li>基于管道的持久化存储（推荐）</li></ul><h4 id=基于终端指令的持久化存储>基于终端指令的持久化存储<a hidden class=anchor aria-hidden=true href=#基于终端指令的持久化存储>#</a></h4><ul><li><p>只可以将parse方法的返回值存储到指定后缀的文本文件中。</p></li><li><p>编码流程：</p><ul><li><p>在爬虫文件中，将爬取到的数据全部封装到parse方法的返回值中</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>DemoSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;demo&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://ishuo.cn/duanzi&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1># 如何获取响应数据</span>
</span></span><span class=line><span class=cl>          <span class=c1># 调用xpath方法对响应数据进行xpath形式的数据解析</span>
</span></span><span class=line><span class=cl>          <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;list&#34;]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=n>all_data</span> <span class=o>=</span> <span class=p>[]</span><span class=c1>#爬取到的数据全部都存储到了该列表中</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[2]/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>content</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[1]/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=c1>#将段子标题和内容封装成parse方法的返回</span>
</span></span><span class=line><span class=cl>              <span class=n>dic</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                  <span class=s1>&#39;title&#39;</span><span class=p>:</span><span class=n>title</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                  <span class=s1>&#39;content&#39;</span><span class=p>:</span><span class=n>content</span>
</span></span><span class=line><span class=cl>              <span class=p>}</span>
</span></span><span class=line><span class=cl>              <span class=n>all_data</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>all_data</span>
</span></span></code></pre></div></li></ul></li><li><p>将parse方法的返回值存储到指定后缀的文本文件中:</p><ul><li>scrapy crawl 爬虫文件名称 -o duanzi.csv</li></ul></li></ul></li><li><p>总结：</p><ul><li>优点：简单，便捷</li><li>缺点：局限性强<ul><li>只可以将数据存储到文本文件无法写入数据库</li><li>存储数据文件后缀是指定好的，通常使用.csv</li><li>需要将存储的数据封装到parse方法的返回值中</li></ul></li></ul></li></ul><h4 id=基于管道实现持久化存储>基于管道实现持久化存储<a hidden class=anchor aria-hidden=true href=#基于管道实现持久化存储>#</a></h4><p>优点：极大程度的提升数据存储的效率</p><p>缺点：编码流程较多</p><h5 id=编码流程>编码流程<a hidden class=anchor aria-hidden=true href=#编码流程>#</a></h5><p>1.在爬虫文件中进行数据解析</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># 如何获取响应数据</span>
</span></span><span class=line><span class=cl>  <span class=c1># 调用xpath方法对响应数据进行xpath形式的数据解析</span>
</span></span><span class=line><span class=cl>  <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;list&#34;]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>all_data</span> <span class=o>=</span> <span class=p>[]</span>  <span class=c1># 爬取到的数据全部都存储到了该列表中</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[2]/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>content</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[1]/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span></code></pre></div><p>2.将解析到的数据封装到Item类型的对象中</p><ul><li><p>2.1 在items.py文件中定义相关的字段</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>SavedataproItem</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Item</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=c1># define the fields for your item here like:</span>
</span></span><span class=line><span class=cl>      <span class=c1># name = scrapy.Field()</span>
</span></span><span class=line><span class=cl>      <span class=c1>#爬取的字段有哪些，这里就需要定义哪些变量存储爬取到的字段</span>
</span></span><span class=line><span class=cl>      <span class=n>title</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Field</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=n>content</span> <span class=o>=</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Field</span><span class=p>()</span>
</span></span></code></pre></div></li></ul></li><li><p>2.2 在爬虫文件中引入Item类，实例化item对象，将解析到的数据存储到item对象中</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      		<span class=kn>from</span> <span class=nn>items</span> <span class=kn>import</span> <span class=n>SavedataproItem</span> <span class=c1>#导入item类</span>
</span></span><span class=line><span class=cl>          <span class=c1># 如何获取响应数据</span>
</span></span><span class=line><span class=cl>          <span class=c1># 调用xpath方法对响应数据进行xpath形式的数据解析</span>
</span></span><span class=line><span class=cl>          <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;list&#34;]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=n>all_data</span> <span class=o>=</span> <span class=p>[]</span>  <span class=c1># 爬取到的数据全部都存储到了该列表中</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[2]/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>content</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[1]/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=c1>#实例化一个item类型的对象</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span> <span class=o>=</span> <span class=n>SavedataproItem</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=c1>#通过中括号的方式访问item对象中的两个成员，且将解析到的两个字段赋值给item对象的两个成员即可</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>content</span>
</span></span></code></pre></div></li></ul></li></ul><p>3.将item对象提交给管道</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1>#将存储好数据的item对象提交给管道</span>
</span></span><span class=line><span class=cl>  <span class=k>yield</span> <span class=n>item</span>
</span></span></code></pre></div></li></ul><p>4.在管道中接收item类型对象(pipelines.py就是管道文件)</p><ul><li><p>管道只可以接收item类型的对象，不可以接收其他类型对象</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>SavedataproPipeline</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1>#process_item用来接收爬虫文件传递过来的item对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#item参数，就是管道接收到的item类型对象</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_item</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>item</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>item</span>
</span></span></code></pre></div></li></ul><p>5.在管道中对接收到的数据进行任意形式的持久化存储操作</p><ul><li><p>可以存储到文件中也可以存储到数据库中</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Define your item pipelines here</span>
</span></span><span class=line><span class=cl>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=c1># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
</span></span><span class=line><span class=cl>  <span class=c1># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>SavedataproPipeline</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1>#重写父类的方法</span>
</span></span><span class=line><span class=cl>      <span class=n>fp</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>open_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;我是open_spider方法，我在项目开始运行环节，只会被执行一次！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>fp</span> <span class=o>=</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;duanzi.txt&#39;</span><span class=p>,</span><span class=s1>&#39;w&#39;</span><span class=p>,</span><span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1>#process_item用来接收爬虫文件传递过来的item对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#item参数，就是管道接收到的item类型对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#process_item方法调用的次数取决于爬虫文件给其提交item的次数</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_item</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#item类型的对象其实就是一个字典</span>
</span></span><span class=line><span class=cl>          <span class=c1># print(item)</span>
</span></span><span class=line><span class=cl>          <span class=c1>#将item字典中的标题和内容获取</span>
</span></span><span class=line><span class=cl>          <span class=n>title</span> <span class=o>=</span> <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=n>content</span> <span class=o>=</span> <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>fp</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>title</span><span class=o>+</span><span class=s1>&#39;:&#39;</span><span class=o>+</span><span class=n>content</span><span class=o>+</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>title</span><span class=p>,</span><span class=s1>&#39;:爬取保存成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>item</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>close_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;在爬虫结束的时候会被执行一次！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>fp</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div></li></ul><p>6.在配置文件中开启管道机制</p><ul><li>注意：默认情况下，管道机制是没有被开启的，需要在配置文件中手动开启</li><li>在setting.py中把ITEM_PIPELINES解除注释就表示开启了管道机制</li></ul><h3 id=管道深入操作>管道深入操作<a hidden class=anchor aria-hidden=true href=#管道深入操作>#</a></h3><ul><li><p>如何将数据存储到数据库</p><ul><li>注意：一个管道类负责将数据存储到一个具体的载体中。如果想要将爬取到的数据存储到多个不同的载体/数据库中，则需要定义多个管道类。</li></ul></li><li><p>思考：</p><ul><li>在有多个管道类的前提下，爬虫文件提交的item会同时给没一个管道类还是单独的管道类？<ul><li>爬虫文件只会将item提交给优先级最高的那一个管道类。优先级最高的管道类的process_item中需要写return item操作，该操作就是表示将item对象传递给下一个管道类，下一个管道类获取了item对象，才可以将数据存储成功！</li></ul></li></ul></li><li><p>管道类：</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Define your item pipelines here</span>
</span></span><span class=line><span class=cl>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=c1># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
</span></span><span class=line><span class=cl>  <span class=c1># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>pymysql</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>redis</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>pymongo</span>
</span></span><span class=line><span class=cl>  <span class=c1>#负责将数据存储到mysql中</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>MysqlPipeline</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>conn</span> <span class=o>=</span> <span class=kc>None</span> <span class=c1>#mysql的链接对象</span>
</span></span><span class=line><span class=cl>      <span class=n>cursor</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>open_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>conn</span> <span class=o>=</span> <span class=n>pymysql</span><span class=o>.</span><span class=n>Connect</span><span class=p>(</span>
</span></span><span class=line><span class=cl>              <span class=n>host</span> <span class=o>=</span> <span class=s1>&#39;127.0.0.1&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>port</span> <span class=o>=</span> <span class=mi>3306</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>user</span> <span class=o>=</span> <span class=s1>&#39;root&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>password</span> <span class=o>=</span> <span class=s1>&#39;boboadmin&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>db</span> <span class=o>=</span> <span class=s1>&#39;spider3qi&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>charset</span> <span class=o>=</span> <span class=s1>&#39;utf8&#39;</span>
</span></span><span class=line><span class=cl>          <span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>cursor</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=o>.</span><span class=n>cursor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=c1>#爬虫文件每向管道提交一个item，则process_item方法就会被调用一次</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_item</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>title</span> <span class=o>=</span> <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=n>sql</span> <span class=o>=</span> <span class=s1>&#39;insert into xiaoshuo (title) values (&#34;</span><span class=si>%s</span><span class=s1>&#34;)&#39;</span><span class=o>%</span><span class=n>title</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>cursor</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=n>sql</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;成功写入一条数据！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>item</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>close_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>cursor</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>#将数据持久化存储到redis中</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>RedisPipeLine</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>conn</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>open_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#在链接前务必手动启动redis的服务</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>conn</span> <span class=o>=</span> <span class=n>redis</span><span class=o>.</span><span class=n>Redis</span><span class=p>(</span>
</span></span><span class=line><span class=cl>              <span class=n>host</span><span class=o>=</span><span class=s1>&#39;127.0.0.1&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>port</span><span class=o>=</span><span class=mi>6379</span>
</span></span><span class=line><span class=cl>          <span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_item</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>item</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#注意：如果想要将一个python字典直接写入到redis中，则redis模块的版本务必是2.10.6</span>
</span></span><span class=line><span class=cl>          <span class=c1>#如果redis模块的版本不是2.10.6则重新安装：pip install redis==2.10.6</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=o>.</span><span class=n>lpush</span><span class=p>(</span><span class=s1>&#39;xiaoshuo&#39;</span><span class=p>,</span><span class=n>item</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;数据存储redis成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>item</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>MongoPipeline</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>conn</span> <span class=o>=</span> <span class=kc>None</span> <span class=c1>#链接对象</span>
</span></span><span class=line><span class=cl>      <span class=n>db_sanqi</span> <span class=o>=</span> <span class=kc>None</span> <span class=c1>#数据仓库</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>open_spider</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>conn</span> <span class=o>=</span> <span class=n>pymongo</span><span class=o>.</span><span class=n>MongoClient</span><span class=p>(</span>
</span></span><span class=line><span class=cl>              <span class=n>host</span><span class=o>=</span><span class=s1>&#39;127.0.0.1&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>port</span><span class=o>=</span><span class=mi>27017</span>
</span></span><span class=line><span class=cl>          <span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>db_sanqi</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=p>[</span><span class=s1>&#39;sanqi&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_item</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>item</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>db_sanqi</span><span class=p>[</span><span class=s1>&#39;xiaoshuo&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>insert_one</span><span class=p>({</span><span class=s1>&#39;title&#39;</span><span class=p>:</span><span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]})</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;插入成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>item</span>
</span></span></code></pre></div></li><li><p>配置文件：</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  ITEM_PIPELINES = {
</span></span><span class=line><span class=cl>     #数字表示管道类被执行的优先级，数字越小表示优先级越高
</span></span><span class=line><span class=cl>     &#39;xiaoshuoPro.pipelines.MysqlPipeline&#39;: 300,
</span></span><span class=line><span class=cl>     &#39;xiaoshuoPro.pipelines.RedisPipeLine&#39;: 301,
</span></span><span class=line><span class=cl>     &#39;xiaoshuoPro.pipelines.MongoPipeline&#39;: 302,
</span></span><span class=line><span class=cl>  }
</span></span></code></pre></div></li></ul><h3 id=scrapy爬取多媒体资源数据>scrapy爬取多媒体资源数据<a hidden class=anchor aria-hidden=true href=#scrapy爬取多媒体资源数据>#</a></h3><ul><li><p>使用一个专有的管道类ImagesPipeline</p></li><li><p>具体的编码流程：</p><ul><li><p>1.在爬虫文件中进行图片/视频的链接提取</p></li><li><p>2.将提取到的链接封装到items对象中，提交给管道</p></li><li><p>3.在管道文件中自定义一个父类为ImagesPipeline的管道类，且重写三个方法即可：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>get_media_requests</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span><span class=n>接收爬虫文件提交过来的item对象</span><span class=err>，</span><span class=n>然后对图片地址发起网路请求</span><span class=err>，</span><span class=n>返回图片的二进制数据</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>file_path</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>response</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>info</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=o>*</span><span class=p>,</span> <span class=n>item</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span><span class=err>：</span><span class=n>指定保存图片的名称</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>item_completed</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>results</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>info</span><span class=p>)</span><span class=err>：</span><span class=n>返回item对象给下一个管道类</span>
</span></span></code></pre></div></li></ul></li><li><p>4.在配置文件中开启指定的管道，且通过IMAGES_STORE = &lsquo;girlsLib&rsquo;操作指定图片存储的文件夹。</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define your item pipelines here</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
</span></span><span class=line><span class=cl><span class=c1># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scrapy.pipelines.images</span> <span class=kn>import</span> <span class=n>ImagesPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#自定义的管道类一定要继承与ImagesPipeline</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>mediaPileline</span><span class=p>(</span><span class=n>ImagesPipeline</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1>#重写三个父类的方法来完成图片二进制数据的请求和持久化存储</span>
</span></span><span class=line><span class=cl>    <span class=c1>#可以根据图片地址，对其进行请求，获取图片数据</span>
</span></span><span class=line><span class=cl>    <span class=c1>#参数item：就是接收到的item对象</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_media_requests</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>img_src</span> <span class=o>=</span> <span class=n>item</span><span class=p>[</span><span class=s1>&#39;src&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>img_src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#指定图片的名称（只需要返回图片存储的名称即可）</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>file_path</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>response</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>info</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=o>*</span><span class=p>,</span> <span class=n>item</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>imgName</span> <span class=o>=</span> <span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;/&#39;</span><span class=p>)[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>imgName</span><span class=p>,</span><span class=s1>&#39;下载保存成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>imgName</span>
</span></span><span class=line><span class=cl>    <span class=c1>#如果没有下一个管道类，该方法可以不写</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>item_completed</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>results</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>item</span> <span class=c1>#可以将当前的管道类接收到item对象传递给下一个管道类2.</span>
</span></span></code></pre></div></li></ul><h3 id=scrapy深度爬取>scrapy深度爬取<a hidden class=anchor aria-hidden=true href=#scrapy深度爬取>#</a></h3><ul><li><p>如何爬取多页的数据（全站数据爬取）</p><ul><li><p>手动请求发送：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1>#callback用来指定解析方法</span>
</span></span><span class=line><span class=cl>  <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>new_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse</span><span class=p>)</span>
</span></span></code></pre></div></li></ul></li></ul></li><li><p>如何爬取深度存储的数据</p><ul><li><p>什么是深度，说白了就是爬取的数据没有存在于同一张页面中。</p></li><li><p>必须使用请求传参的机制才可以完整的实现。</p><ul><li><p>请求传参：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>meta</span><span class=o>=</span><span class=p>{},</span><span class=n>url</span><span class=o>=</span><span class=n>detail_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse_detail</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>可以将meta字典传递给callback这个回调函数</span>
</span></span></code></pre></div></li></ul></li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>..items</span> <span class=kn>import</span> <span class=n>DeepproItem</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DeepSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;deep&#39;</span>
</span></span><span class=line><span class=cl>    <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>    <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://wz.sun0769.com/political/index/politicsNewest&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1>#解析首页数据</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;/html/body/div[2]/div[3]/ul[2]/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./span[3]/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>detail_url</span> <span class=o>=</span> <span class=s1>&#39;https://wz.sun0769.com&#39;</span><span class=o>+</span><span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./span[3]/a/@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=c1># print(title)</span>
</span></span><span class=line><span class=cl>            <span class=n>item</span> <span class=o>=</span> <span class=n>DeepproItem</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>            <span class=c1>#对详情页的url发起请求</span>
</span></span><span class=line><span class=cl>            <span class=c1>#参数meta可以将自身这个字典传递给callback指定的回调函数</span>
</span></span><span class=line><span class=cl>            <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;item&#39;</span><span class=p>:</span><span class=n>item</span><span class=p>},</span><span class=n>url</span><span class=o>=</span><span class=n>detail_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse_detail</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#解析详情页数据</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>parse_detail</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>meta</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>meta</span> <span class=c1>#接收请求传参过来的meta字典</span>
</span></span><span class=line><span class=cl>        <span class=n>item</span> <span class=o>=</span> <span class=n>meta</span><span class=p>[</span><span class=s1>&#39;item&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>content</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;/html/body/div[3]/div[2]/div[2]/div[2]//text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>content</span> <span class=o>=</span> <span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># print(content)</span>
</span></span><span class=line><span class=cl>        <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>content</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>yield</span> <span class=n>item</span>
</span></span></code></pre></div><h3 id=imagepipelines的请求传参>ImagePipeLines的请求传参<a hidden class=anchor aria-hidden=true href=#imagepipelines的请求传参>#</a></h3><ul><li><p>环境安装：pip install Pillow</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  USER_AGENT = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36&#39;
</span></span></code></pre></div></li><li><p>需求：将图片的名称和详情页中图片的数据进行爬取，持久化存储。</p><ul><li><p>分析：</p><ul><li>深度爬取：请求传参</li><li>多页的数据爬取：手动请求的发送</li></ul></li><li><p>爬虫文件：</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>..items</span> <span class=kn>import</span> <span class=n>DeepimgproItem</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>ImgSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;img&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://pic.netbian.com/4kmeinv/&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=c1>#通用的url模板</span>
</span></span><span class=line><span class=cl>      <span class=n>url_model</span> <span class=o>=</span> <span class=s1>&#39;https://pic.netbian.com/4kmeinv/index_</span><span class=si>%d</span><span class=s1>.html&#39;</span>
</span></span><span class=line><span class=cl>      <span class=n>page_num</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#解析出了图片的名称和详情页的url</span>
</span></span><span class=line><span class=cl>          <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;main&#34;]/div[3]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./a/b/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span> <span class=o>+</span> <span class=s1>&#39;.jpg&#39;</span>
</span></span><span class=line><span class=cl>              <span class=n>detail_url</span> <span class=o>=</span> <span class=s1>&#39;https://pic.netbian.com&#39;</span><span class=o>+</span><span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./a/@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span> <span class=o>=</span> <span class=n>DeepimgproItem</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>              <span class=c1>#需要对详情页的url发起请求，在详情页中获取图片的下载链接</span>
</span></span><span class=line><span class=cl>              <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>detail_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>detail_parse</span><span class=p>,</span><span class=n>meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;item&#39;</span><span class=p>:</span><span class=n>item</span><span class=p>})</span>
</span></span><span class=line><span class=cl>          <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>page_num</span> <span class=o>&lt;=</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>new_url</span> <span class=o>=</span> <span class=nb>format</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>url_model</span><span class=o>%</span><span class=bp>self</span><span class=o>.</span><span class=n>page_num</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=bp>self</span><span class=o>.</span><span class=n>page_num</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>              <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>new_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1>#解析详情页的数据</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>detail_parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>meta</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>meta</span>
</span></span><span class=line><span class=cl>          <span class=n>item</span> <span class=o>=</span> <span class=n>meta</span><span class=p>[</span><span class=s1>&#39;item&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=n>img_src</span> <span class=o>=</span> <span class=s1>&#39;https://pic.netbian.com&#39;</span><span class=o>+</span><span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;img&#34;]/img/@src&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=n>item</span><span class=p>[</span><span class=s1>&#39;img_src&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>img_src</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          <span class=k>yield</span> <span class=n>item</span>
</span></span></code></pre></div></li><li><p>管道：</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Define your item pipelines here</span>
</span></span><span class=line><span class=cl>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=c1># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
</span></span><span class=line><span class=cl>  <span class=c1># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>scrapy.pipelines.images</span> <span class=kn>import</span> <span class=n>ImagesPipeline</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>DeepimgproPipeline</span><span class=p>(</span><span class=n>ImagesPipeline</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=c1># def process_item(self, item, spider):</span>
</span></span><span class=line><span class=cl>      <span class=c1>#     return item</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>get_media_requests</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>img_src</span> <span class=o>=</span> <span class=n>item</span><span class=p>[</span><span class=s1>&#39;img_src&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=c1>#请求传参，将item中的图片名称传递给file_path方法</span>
</span></span><span class=line><span class=cl>          <span class=c1>#meta会将自身传递给file_path</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>],</span><span class=s1>&#39;保存下载成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>img_src</span><span class=p>,</span><span class=n>meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;title&#39;</span><span class=p>:</span><span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]})</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>file_path</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>response</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>info</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=o>*</span><span class=p>,</span> <span class=n>item</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#返回图片的名称</span>
</span></span><span class=line><span class=cl>          <span class=c1>#接收请求传参过来的数据</span>
</span></span><span class=line><span class=cl>          <span class=n>title</span> <span class=o>=</span> <span class=n>request</span><span class=o>.</span><span class=n>meta</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>item_completed</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>results</span><span class=p>,</span> <span class=n>item</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>item</span>
</span></span></code></pre></div></li></ul></li></ul><h3 id=如何提高scrapy的爬取效率>如何提高scrapy的爬取效率<a hidden class=anchor aria-hidden=true href=#如何提高scrapy的爬取效率>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>增加并发：
</span></span><span class=line><span class=cl>    默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>降低日志级别：
</span></span><span class=line><span class=cl>    在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为WORNING或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘ERROR’
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>禁止cookie：
</span></span><span class=line><span class=cl>    如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>禁止重试：
</span></span><span class=line><span class=cl>    对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>减少下载超时：
</span></span><span class=line><span class=cl>    如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s
</span></span></code></pre></div><h3 id=post请求发送>post请求发送<a hidden class=anchor aria-hidden=true href=#post请求发送>#</a></h3><ul><li><p>问题：在之前代码中，我们从来没有手动的对start_urls列表中存储的起始url进行过请求的发送，但是起始url的确是进行了请求的发送，那这是如何实现的呢？</p><ul><li><p>解答：其实是因为爬虫文件中的爬虫类继承到了Spider父类中的start_requests（self）这个方法，该方法就可以对start_urls列表中的url发起请求：</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>start_requests</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>u</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>start_urls</span><span class=p>:</span>
</span></span><span class=line><span class=cl>           <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>u</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p>【注意】该方法默认的实现，是对起始的url发起get请求，如果想发起post请求，则需要子类重写该方法。</p><ul><li>yield scrapy.Request():发起get请求</li><li>yield scrapy.FormRequest():发起post请求</li></ul></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>FanyiSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;fanyi&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://fanyi.baidu.com/sug&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=c1>#父类中的方法：该方法是用来给起始的url列表中的每一个url发请求</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>start_requests</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>              <span class=s1>&#39;kw&#39;</span><span class=p>:</span><span class=s1>&#39;dog&#39;</span>
</span></span><span class=line><span class=cl>          <span class=p>}</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>url</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>start_urls</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=c1>#formdata是用来指定请求参数</span>
</span></span><span class=line><span class=cl>              <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>FormRequest</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse</span><span class=p>,</span><span class=n>formdata</span><span class=o>=</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>result</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div></li></ul></li></ul><h3 id=scrapy的核心组件>scrapy的核心组件<a hidden class=anchor aria-hidden=true href=#scrapy的核心组件>#</a></h3><ul><li>从中可以大致了解scrapy框架的一个运行机制</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=o>-</span> <span class=err>引擎</span><span class=p>(</span><span class=n>Scrapy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=err>用来处理整个系统的数据流处理</span><span class=p>,</span> <span class=err>触发事务</span><span class=p>(</span><span class=err>框架核心</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=err>调度器</span><span class=p>(</span><span class=n>Scheduler</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=err>用来接受引擎发过来的请求</span><span class=p>,</span> <span class=err>压入队列中</span><span class=p>,</span> <span class=err>并在引擎再次请求的时候返回</span><span class=o>.</span> <span class=err>可以想像成一个</span><span class=n>URL</span><span class=err>（抓取网页的网址或者说是链接）的优先队列</span><span class=p>,</span> <span class=err>由它来决定下一个要抓取的网址是什么</span><span class=p>,</span> <span class=err>同时去除重复的网址</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=err>下载器</span><span class=p>(</span><span class=n>Downloader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=err>用于下载网页内容</span><span class=p>,</span> <span class=err>并将网页内容返回给蜘蛛</span><span class=p>(</span><span class=n>Scrapy下载器是建立在twisted这个高效的异步模型上的</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=err>爬虫</span><span class=p>(</span><span class=n>Spiders</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=err>爬虫是主要干活的</span><span class=p>,</span> <span class=err>用于从特定的网页中提取自己需要的信息</span><span class=p>,</span> <span class=err>即所谓的实体</span><span class=p>(</span><span class=n>Item</span><span class=p>)</span><span class=err>。用户也可以从中提取出链接</span><span class=p>,</span><span class=err>让</span><span class=n>Scrapy继续抓取下一个页面</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=err>项目管道</span><span class=p>(</span><span class=n>Pipeline</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=err>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</span>
</span></span></code></pre></div><h3 id=中间件>中间件<a hidden class=anchor aria-hidden=true href=#中间件>#</a></h3><ul><li><p>scrapy的中间件有两个：</p><ul><li>爬虫中间件</li><li>下载中间件</li><li>中间件的作用是什么？<ul><li>观测中间件在五大核心组件的什么位置，根据位置了解中间件的作用<ul><li>下载中间件位于引擎和下载器之间</li><li>引擎会给下载器传递请求对象，下载器会给引擎返回响应对象。</li><li>作用：可以拦截到scrapy框架中所有的请求和响应。<ul><li>拦截请求干什么？<ul><li>修改请求的ip，修改请求的头信息，设置请求的cookie</li></ul></li><li>拦截响应干什么？<ul><li>可以修改响应数据</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>中间件重要方法：</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Define here the models for your spider middleware</span>
</span></span><span class=line><span class=cl>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=c1># See documentation in:</span>
</span></span><span class=line><span class=cl>  <span class=c1># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>scrapy</span> <span class=kn>import</span> <span class=n>signals</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>is_item</span><span class=p>,</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>MiddleproDownloaderMiddleware</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1>#类方法：作用是返回一个下载器对象（忽略）</span>
</span></span><span class=line><span class=cl>      <span class=nd>@classmethod</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>from_crawler</span><span class=p>(</span><span class=bp>cls</span><span class=p>,</span> <span class=n>crawler</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1># This method is used by Scrapy to create your spiders.</span>
</span></span><span class=line><span class=cl>          <span class=n>s</span> <span class=o>=</span> <span class=bp>cls</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=n>crawler</span><span class=o>.</span><span class=n>signals</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=n>s</span><span class=o>.</span><span class=n>spider_opened</span><span class=p>,</span> <span class=n>signal</span><span class=o>=</span><span class=n>signals</span><span class=o>.</span><span class=n>spider_opened</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>s</span>
</span></span><span class=line><span class=cl>      <span class=c1>#拦截处理所有的请求对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数：request就是拦截到的请求对象，spider爬虫文件中爬虫类实例化的对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#spider参数的作用可以实现爬虫类和中间类的数据交互</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_request</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>      <span class=c1>#拦截处理所有的响应对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数：response就是拦截到的响应对象，request就是被拦截到响应对象对应的唯一的一个请求对象</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_response</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>response</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>response</span>
</span></span><span class=line><span class=cl>      <span class=c1>#拦截和处理发生异常的请求对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数：reqeust就是拦截到的发生异常的请求对象</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_exception</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>exception</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=k>pass</span>
</span></span><span class=line><span class=cl>      <span class=c1>#控制日志数据的（忽略）</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>spider_opened</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>spider</span><span class=o>.</span><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s1>&#39;Spider opened: </span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>spider</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
</span></span></code></pre></div></li></ul><h4 id=开发代理中间件>开发代理中间件<a hidden class=anchor aria-hidden=true href=#开发代理中间件>#</a></h4><ul><li><p>request.meta[&lsquo;proxy&rsquo;] = proxy</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Define here the models for your spider middleware</span>
</span></span><span class=line><span class=cl>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=c1># See documentation in:</span>
</span></span><span class=line><span class=cl>  <span class=c1># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>scrapy</span> <span class=kn>import</span> <span class=n>signals</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>is_item</span><span class=p>,</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>scrapy</span> <span class=kn>import</span> <span class=n>Request</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>MiddleproDownloaderMiddleware</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1>#类方法：作用是返回一个下载器对象（忽略）</span>
</span></span><span class=line><span class=cl>      <span class=nd>@classmethod</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>from_crawler</span><span class=p>(</span><span class=bp>cls</span><span class=p>,</span> <span class=n>crawler</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1># This method is used by Scrapy to create your spiders.</span>
</span></span><span class=line><span class=cl>          <span class=n>s</span> <span class=o>=</span> <span class=bp>cls</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=n>crawler</span><span class=o>.</span><span class=n>signals</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=n>s</span><span class=o>.</span><span class=n>spider_opened</span><span class=p>,</span> <span class=n>signal</span><span class=o>=</span><span class=n>signals</span><span class=o>.</span><span class=n>spider_opened</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>s</span>
</span></span><span class=line><span class=cl>      <span class=c1>#拦截处理所有的请求对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数：request就是拦截到的请求对象，spider爬虫文件中爬虫类实例化的对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#spider参数的作用可以实现爬虫类和中间类的数据交互</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_request</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#是的所有的请求都是用代理，则代理操作可以写在该方法中</span>
</span></span><span class=line><span class=cl>          <span class=n>request</span><span class=o>.</span><span class=n>meta</span><span class=p>[</span><span class=s1>&#39;proxy&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;http://ip:port&#39;</span>
</span></span><span class=line><span class=cl>          <span class=c1>#弊端：会使得整体的请求效率变低</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=o>+</span><span class=s1>&#39;:请求对象拦截成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>      <span class=c1>#拦截处理所有的响应对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数：response就是拦截到的响应对象，request就是被拦截到响应对象对应的唯一的一个请求对象</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_response</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>response</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=o>+</span><span class=s1>&#39;:响应对象拦截成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>response</span>
</span></span><span class=line><span class=cl>      <span class=c1>#拦截和处理发生异常的请求对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#参数：reqeust就是拦截到的发生异常的请求对象</span>
</span></span><span class=line><span class=cl>      <span class=c1>#方法存在的意义：将发生异常的请求拦截到，然后对其进行修正</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_exception</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>exception</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=o>+</span><span class=s1>&#39;:发生异常的请求对象被拦截到！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=c1>#修正操作</span>
</span></span><span class=line><span class=cl>          <span class=c1>#只有发生了异常的请求才使用代理机制，则可以写在该方法中</span>
</span></span><span class=line><span class=cl>          <span class=n>request</span><span class=o>.</span><span class=n>meta</span><span class=p>[</span><span class=s1>&#39;proxy&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;https://ip:port&#39;</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=n>request</span> <span class=c1>#对请求对象进行重新发送</span>
</span></span><span class=line><span class=cl>      <span class=c1>#控制日志数据的（忽略）</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>spider_opened</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>spider</span><span class=o>.</span><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s1>&#39;Spider opened: </span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>spider</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
</span></span></code></pre></div></li></ul><h4 id=开发ua中间件>开发UA中间件<a hidden class=anchor aria-hidden=true href=#开发ua中间件>#</a></h4><ul><li><p>request.headers[&lsquo;User-Agent&rsquo;] = ua</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_request</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>request</span><span class=o>.</span><span class=n>headers</span><span class=p>[</span><span class=s1>&#39;User-Agent&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;从列表中随机选择的一个UA值&#39;</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=o>+</span><span class=s1>&#39;:请求对象拦截成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=kc>None</span>
</span></span></code></pre></div></li></ul><h4 id=开发cookie中间件>开发Cookie中间件<a hidden class=anchor aria-hidden=true href=#开发cookie中间件>#</a></h4><ul><li><p>request.cookies = cookies</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>process_request</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>request</span><span class=o>.</span><span class=n>headers</span><span class=p>[</span><span class=s1>&#39;cookie&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;xxx&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1>#request.cookies = &#39;xxx&#39;</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=o>+</span><span class=s1>&#39;:请求对象拦截成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=kc>None</span>
</span></span></code></pre></div></li></ul><h4 id=seleniumscrapy>selenium+scrapy<a hidden class=anchor aria-hidden=true href=#seleniumscrapy>#</a></h4><ul><li><p>需求：将网易新闻中的国内，国际，军事，航空四个板块下的新闻标题和内容进行数据爬取</p><ul><li>注意：哪些数据是动态加载的！</li><li>技术：selenium，scrapy，中间件</li></ul></li><li><p>分析：</p><ul><li>抓取首页中四个板块下所有的新闻标题和新闻内容<ul><li>获取首页中四个板块对应的详情页链接<ul><li>首页是没有动态加载数据，可以直接爬取+解析</li></ul></li></ul></li><li>对每一个板块的url发起请求，获取详情页中的新闻标题等内容<ul><li>通过分析发现每一个板块中的新闻数据全部是动态加载的数据，如何解决呢？<ul><li>通过selenium解决</li></ul></li></ul></li></ul></li><li><p>scrapy+selenium的编码流程</p><ul><li>1.在爬虫文件中定义浏览器对象，将浏览器对象作为爬虫类的一个成员变量</li><li>2.在中间件中通过spider获取爬虫文件中定义的浏览器对象，进行请求发送和获取响应数据</li><li>3.在爬虫文件中重写一个closed方法，来关闭浏览器对象</li></ul></li><li><p>爬虫文件</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>..items</span> <span class=kn>import</span> <span class=n>WangyiproItem</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>selenium</span> <span class=kn>import</span> <span class=n>webdriver</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>WangyiSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;wangyi&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://news.163.com/&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=c1>#创建浏览器对象，把浏览器对象作为爬虫类的一个成员</span>
</span></span><span class=line><span class=cl>      <span class=n>bro</span> <span class=o>=</span> <span class=n>webdriver</span><span class=o>.</span><span class=n>Chrome</span><span class=p>(</span><span class=n>executable_path</span><span class=o>=</span><span class=s1>&#39;/Users/zhangxiaobo/Desktop/三期/chromedriver1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=n>model_urls</span> <span class=o>=</span> <span class=p>[]</span> <span class=c1>#存储4个板块对应的url</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#从首页解析每一个板块对应详情页的url，将其存储到model_urls列表中</span>
</span></span><span class=line><span class=cl>          <span class=n>model_index</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>5</span><span class=p>,</span><span class=mi>6</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;index2016_wrap&#34;]/div[3]/div[2]/div[2]/div[2]/div/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>index</span> <span class=ow>in</span> <span class=n>model_index</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>model_url</span> <span class=o>=</span> <span class=n>li_list</span><span class=p>[</span><span class=n>index</span><span class=p>]</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./a/@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=bp>self</span><span class=o>.</span><span class=n>model_urls</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>model_url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=c1>#应该对每一个板块的详情页发起请求（动态加载）</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>model_url</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model_urls</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>model_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse_detail</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1>#目的是为了解析出每一个板块中的新闻标题和新闻详情页的url</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse_detail</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#response就是一个不符合需求要求的响应对象</span>
</span></span><span class=line><span class=cl>              <span class=c1>#该response中没有存储动态加载的新闻数据，因此该响应对象被视为不符合要求的响应对象</span>
</span></span><span class=line><span class=cl>              <span class=c1>#需要将不符合要求的响应对象变为符合要求的响应对象即可，如何做呢？</span>
</span></span><span class=line><span class=cl>              <span class=c1>#方法：篡改不符合要求的响应对象的响应数据，将该响应对象的响应数据修改为包含了动态加载的新闻数据即可。</span>
</span></span><span class=line><span class=cl>          <span class=n>div_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;/html/body/div/div[3]/div[4]/div[1]/div[1]/div/ul/li/div/div&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>div</span> <span class=ow>in</span> <span class=n>div_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                  <span class=c1>#解析新闻标题+新闻详情页的url</span>
</span></span><span class=line><span class=cl>                  <span class=n>title</span> <span class=o>=</span> <span class=n>div</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div/div[1]/h3/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                  <span class=n>new_detail_url</span> <span class=o>=</span> <span class=n>div</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div/div[1]/h3/a/@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                  <span class=n>item</span> <span class=o>=</span> <span class=n>WangyiproItem</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                  <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>              <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;遇到了广告，忽略此次行为即可！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>              <span class=c1>#对新闻的详情页发起请求</span>
</span></span><span class=line><span class=cl>              <span class=k>if</span> <span class=n>new_detail_url</span> <span class=o>!=</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                  <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>new_detail_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>new_content_parse</span><span class=p>,</span><span class=n>meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;item&#39;</span><span class=p>:</span><span class=n>item</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>new_content_parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>item</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>meta</span><span class=p>[</span><span class=s1>&#39;item&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=c1>#解析新闻的详情内容</span>
</span></span><span class=line><span class=cl>          <span class=n>content</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;content&#34;]/div[2]//text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=n>content</span> <span class=o>=</span> <span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>content</span><span class=p>)</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>content</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          <span class=k>yield</span> <span class=n>item</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1>#重写一个父类方法，close_spider，该方法只会在爬虫最后执行一次</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>closed</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#关闭浏览器</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;关闭浏览器成功！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=bp>self</span><span class=o>.</span><span class=n>bro</span><span class=o>.</span><span class=n>quit</span><span class=p>()</span>
</span></span></code></pre></div></li></ul></li><li><p>中间件文件：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Define here the models for your spider middleware</span>
</span></span><span class=line><span class=cl>  <span class=c1>#</span>
</span></span><span class=line><span class=cl>  <span class=c1># See documentation in:</span>
</span></span><span class=line><span class=cl>  <span class=c1># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>scrapy</span> <span class=kn>import</span> <span class=n>signals</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># useful for handling different item types with a single interface</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>itemadapter</span> <span class=kn>import</span> <span class=n>is_item</span><span class=p>,</span> <span class=n>ItemAdapter</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>time</span> <span class=kn>import</span> <span class=n>sleep</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>scrapy.http</span> <span class=kn>import</span> <span class=n>HtmlResponse</span><span class=c1>#scrapy封装的响应对象对应的类</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>WangyiproDownloaderMiddleware</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_request</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_response</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>response</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=c1>#可以拦截到所有的响应对象</span>
</span></span><span class=line><span class=cl>          <span class=c1>#当前项目一共会产生多少个响应对象呢？</span>
</span></span><span class=line><span class=cl>           <span class=c1>#1 + 4 + n个响应对象，在这些响应对象中只有4这4个响应对象需要被修改</span>
</span></span><span class=line><span class=cl>          <span class=c1>#如何筛选出指定的4个板块对应的响应对象呢？</span>
</span></span><span class=line><span class=cl>              <span class=c1>#1.可以先找出指定4个板块的请求对象，然后根据请求对象定位指定4个响应对象</span>
</span></span><span class=line><span class=cl>              <span class=c1>#2.可以根据4个板块的url定位到四个板块的请求对象</span>
</span></span><span class=line><span class=cl>          <span class=n>model_urls</span> <span class=o>=</span> <span class=n>spider</span><span class=o>.</span><span class=n>model_urls</span>
</span></span><span class=line><span class=cl>          <span class=k>if</span> <span class=n>request</span><span class=o>.</span><span class=n>url</span> <span class=ow>in</span> <span class=n>model_urls</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>bro</span> <span class=o>=</span> <span class=n>spider</span><span class=o>.</span><span class=n>bro</span> <span class=c1>#从爬虫类中获取创建好的浏览器对象</span>
</span></span><span class=line><span class=cl>              <span class=n>bro</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=n>sleep</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=c1># bro.execute_script(&#39;document.documentElement.scrollTo(0,9000)&#39;)</span>
</span></span><span class=line><span class=cl>              <span class=c1># sleep(1)</span>
</span></span><span class=line><span class=cl>              <span class=c1>#获取动态加载的数据</span>
</span></span><span class=line><span class=cl>              <span class=n>page_text</span> <span class=o>=</span> <span class=n>bro</span><span class=o>.</span><span class=n>page_source</span>
</span></span><span class=line><span class=cl>              <span class=c1>#说明该request就是指定响应对象的请求对象</span>
</span></span><span class=line><span class=cl>              <span class=c1>#此处的response就是指定板块对应的响应对象</span>
</span></span><span class=line><span class=cl>              <span class=n>response</span> <span class=o>=</span> <span class=n>HtmlResponse</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>request</span><span class=o>.</span><span class=n>url</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>request</span><span class=o>=</span><span class=n>request</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>body</span><span class=o>=</span><span class=n>page_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                                  <span class=c1>#body就是响应对象的响应数据</span>
</span></span><span class=line><span class=cl>              <span class=k>return</span> <span class=n>response</span>
</span></span><span class=line><span class=cl>          <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=k>return</span> <span class=n>response</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>process_exception</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>request</span><span class=p>,</span> <span class=n>exception</span><span class=p>,</span> <span class=n>spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>         <span class=k>pass</span>
</span></span></code></pre></div></li></ul></li><li><p>配置文件：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=n>DOWNLOADER_MIDDLEWARES</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;wangyiPro.middlewares.WangyiproDownloaderMiddleware&#39;</span><span class=p>:</span> <span class=mi>543</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div></li></ul></li><li><p>拓展功能：将人工智能+数据爬取中</p><ul><li><p>实现将爬取到的新闻进行分类和关键字提取</p><ul><li><p>百度AI的使用：https://ai.baidu.com/</p><ul><li><p>使用流程：</p><ul><li><p>点击首页右上角的控制台，进行登录。</p></li><li><p>登录后进入到了智能云的首页</p><ul><li><p>点击页面左上角的三条杠，选择你想要实现的功能，点击，进入到指定功能页面</p><ul><li>在功能页面，首先点击【创建应用】，进行应用的创建</li><li>创建好之后，点击管理应用就可以看到：<ul><li>AppID，apiKey，secret key这三个值，会在程序中用到</li></ul></li></ul></li><li><p>在功能页面点击左侧的【技术文档】，选择SDK说明，选择对应的Python语言即可，先看快速开始内容，在选择你想要实现的具体功能的文档界面即可。</p><ul><li>环境安装：pip install baidu-aip</li></ul></li><li><p>提取文章关键字：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>aip</span> <span class=kn>import</span> <span class=n>AipNlp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34; 你的 APPID AK SK &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>APP_ID</span> <span class=o>=</span> <span class=s1>&#39;xxx&#39;</span>
</span></span><span class=line><span class=cl>  <span class=n>API_KEY</span> <span class=o>=</span> <span class=s1>&#39;xxx&#39;</span>
</span></span><span class=line><span class=cl>  <span class=n>SECRET_KEY</span> <span class=o>=</span> <span class=s1>&#39;xxx&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>client</span> <span class=o>=</span> <span class=n>AipNlp</span><span class=p>(</span><span class=n>APP_ID</span><span class=p>,</span> <span class=n>API_KEY</span><span class=p>,</span> <span class=n>SECRET_KEY</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>title</span> <span class=o>=</span> <span class=s2>&#34;iphone手机出现“白苹果”原因及解决办法，用苹果手机的可以看下&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>content</span> <span class=o>=</span> <span class=s2>&#34;如果下面的方法还是没有解决你的问题建议来我们门店看下成都市锦江区红星路三段99号银石广场24层01室。&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34; 调用文章标签 &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>result</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>keyword</span><span class=p>(</span><span class=n>title</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>dic</span> <span class=ow>in</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;items&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=n>dic</span><span class=p>[</span><span class=s1>&#39;score&#39;</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=mf>0.8</span><span class=p>:</span>
</span></span><span class=line><span class=cl>          <span class=n>key</span> <span class=o>=</span> <span class=n>dic</span><span class=p>[</span><span class=s1>&#39;tag&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=n>key</span><span class=p>)</span>
</span></span></code></pre></div></li></ul></li><li><p>文章分类：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>aip</span> <span class=kn>import</span> <span class=n>AipNlp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34; 你的 APPID AK SK &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>APP_ID</span> <span class=o>=</span> <span class=s1>&#39;xxx&#39;</span>
</span></span><span class=line><span class=cl>  <span class=n>API_KEY</span> <span class=o>=</span> <span class=s1>&#39;x&#39;</span>
</span></span><span class=line><span class=cl>  <span class=n>SECRET_KEY</span> <span class=o>=</span> <span class=s1>&#39;xx&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>client</span> <span class=o>=</span> <span class=n>AipNlp</span><span class=p>(</span><span class=n>APP_ID</span><span class=p>,</span> <span class=n>API_KEY</span><span class=p>,</span> <span class=n>SECRET_KEY</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>title</span> <span class=o>=</span> <span class=s2>&#34;秦刚访问特斯拉美国工厂马斯克陪同 传递什么信号？&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>content</span> <span class=o>=</span> <span class=s2>&#34;今天（4日），驻美大使秦刚访问了特斯拉硅谷工厂，同特斯拉CEO马斯克针对各项尖端科技、人类未来等主题展开探讨，并体验了特斯拉的新款Model S及最新自动辅助驾驶系统。他在海外社交平台上表示：“性能强劲，但乘坐平顺舒适”。&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34; 调用文章分类 &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>result</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>topic</span><span class=p>(</span><span class=n>title</span><span class=p>,</span> <span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>class_new</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;item&#39;</span><span class=p>][</span><span class=s1>&#39;lv1_tag_list&#39;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;tag&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>class_new</span><span class=p>)</span>
</span></span></code></pre></div></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id=crawlspider>CrawlSpider<a hidden class=anchor aria-hidden=true href=#crawlspider>#</a></h3><ul><li><p>实现网站的全站数据爬取</p><ul><li>就是将网站中所有页码对应的页面数据进行爬取。</li></ul></li><li><p>crawlspider其实就是scrapy封装好的一个爬虫类，通过该类提供的相关的方法和属性就可以实现全新高效形式的全站数据爬取。</p></li><li><p>使用流程：</p><ul><li><p>新建一个scrapy项目</p></li><li><p>cd 项目</p></li><li><p>创建爬虫文件（*）：</p><ul><li><p>scrapy genspider-t crawl spiderName <a href=https://www.xxx.com>www.xxx.com</a></p></li><li><p>爬虫文件中发生的变化有哪些？</p><ul><li>当前爬虫类的父类为CrawlSpider</li><li>爬虫类中多了一个类变量叫做rules<ul><li>LinkExtractor：链接提取器<ul><li>可以根据allow参数表示的正则在当前页面中提取符合正则要求的链接</li></ul></li><li>Rule：规则解析器<ul><li>可以接收链接提取器提取到的链接，并且对每一个链接进行请求发送</li><li>可以根据callback指定的回调函数对每一次请求到的数据进行数据解析</li></ul></li></ul></li><li>思考:如何将一个网站中所有的链接都提取到呢？<ul><li>只需要在链接提取器的allow后面赋值一个空正则表达式即可</li></ul></li><li>目前在scrapy中有几种发送请求的方式？<ul><li>start_urls列表可以发送请求</li><li>scrapy.Request()</li><li>scrapy.FormRequest()</li><li>Rule规则解析器</li></ul></li></ul></li></ul></li></ul></li><li><p>注意：</p><ul><li>链接提取器和规则解析器是一一对应的（一对一的关系）</li><li>建议在使用crawlSpider实现深度爬取的时候，需要配合手动请求发送的方式进行搭配！</li></ul></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  USER_AGENT = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36&#39;
</span></span></code></pre></div></li></ul><h3 id=分布式>分布式<a hidden class=anchor aria-hidden=true href=#分布式>#</a></h3><ul><li><p>分布式在日常开发中并不常用，只是一个噱头！</p></li><li><p>概念：</p><ul><li>可以使用多台电脑搭建一个分布式机群，使得多台对电脑可以对同一个网站的数据进行联合且分布的数据爬取。</li></ul></li><li><p>声明：</p><ul><li>原生的scrapy框架并无法实现分布式操作！why？<ul><li>多台电脑之间无法共享同一个调度器</li><li>多台电脑之间无法共享同一个管道</li></ul></li></ul></li><li><p>如何是的scrapy可以实现分布式呢？</p><ul><li>借助于一个组件：scrapy-redis</li><li>scrapy-redis的作用是什么？<ul><li>可以给原生的scrapy框架提供可被共享的调度器和管道！</li><li>环境安装：pip install scrapy-redis<ul><li>注意：scrapy-redis该组件只可以将爬取到的数据存储到redis数据库</li></ul></li></ul></li></ul></li><li><p>编码流程（重点）：</p><ul><li><p>1.创建项目</p></li><li><p>2.cd 项目</p></li><li><p>3.创建基于crawlSpider的爬虫文件</p><ul><li>3.1 修改爬虫文件<ul><li>导包：from scrapy_redis.spiders import RedisCrawlSpider</li><li>修改当前爬虫类的父类为 RedisCrawlSpider</li><li>将start_urls替换成redis_key的操作<ul><li>redis_key变量的赋值为字符串，该字符串表示调度器队列的名称</li></ul></li><li>进行常规的请求操作和数据解析</li></ul></li></ul></li><li><p>4.settings配置文件的修改</p><ul><li><p>常规内容修改（robots和ua等），先不指定日志等级</p></li><li><p>指定可以被共享的管道类</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  ITEM_PIPELINES = {
</span></span><span class=line><span class=cl>      &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400
</span></span><span class=line><span class=cl>  }
</span></span></code></pre></div></li></ul></li><li><p>指定可以被共享的调度器</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  # 使用scrapy-redis组件的去重队列
</span></span><span class=line><span class=cl>  DUPEFILTER_CLASS = &#34;scrapy_redis.dupefilter.RFPDupeFilter&#34;
</span></span><span class=line><span class=cl>  # 使用scrapy-redis组件自己的调度器
</span></span><span class=line><span class=cl>  SCHEDULER = &#34;scrapy_redis.scheduler.Scheduler&#34;
</span></span><span class=line><span class=cl>  # 是否允许暂停
</span></span><span class=line><span class=cl>  SCHEDULER_PERSIST = True
</span></span></code></pre></div></li></ul></li><li><p>指定数据库</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  REDIS_HOST = &#39;127.0.0.1&#39;
</span></span><span class=line><span class=cl>  REDIS_PORT = 6379
</span></span></code></pre></div></li></ul></li></ul></li><li><p>5.修改redis数据库的配置文件（redis.windows.conf）</p><ul><li><p>在配置文件中改行代码是没有没注释的：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  bind 127.0.0.1
</span></span><span class=line><span class=cl>  #将上述代码注释即可（解除本机绑定，实现外部设备访问本机数据库
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  如果配置文件中还存在：protected-mode = true，将true修改为false，
</span></span><span class=line><span class=cl>  修改为false后表示redis数据库关闭了保护模式，表示其他设备可以远程访问且修改你数据库中的数据
</span></span></code></pre></div></li></ul></li></ul></li><li><p>6.启动redis数据库的服务端和客户端</p></li><li><p>7.运行项目,发现程序暂定一直在等待，等待爬取任务</p></li><li><p>8.需要向可以被共享的调度器的队列（redis_key的值）中放入一个起始的url</p><ul><li><p>在redis数据库的客户端执行如下操作：</p><ul><li>lpush 队列名称 起始的url</li><li>起始url：https://wz.sun0769.com/political/index/politicsNewest?id=1&amp;page=1</li></ul></li></ul></li></ul></li></ul><h3 id=增量式>增量式<a hidden class=anchor aria-hidden=true href=#增量式>#</a></h3><ul><li><p>爬虫应用场景分类</p><ul><li>通用爬虫</li><li>聚焦爬虫</li><li>功能爬虫</li><li>分布式爬虫</li><li>增量式：<ul><li>用来监测网站数据更新的情况（爬取网站最新更新出来的数据）。</li><li>只是一种程序设计的思路，使用什么技术都是可以实现的。</li><li>核心：<ul><li>去重。<ul><li>使用一个记录表来实现数据的去重：<ul><li>记录表：存储爬取过的数据的记录</li><li>如何构建和设计一个记录表：<ul><li>记录表需要具备的特性：<ul><li>去重</li><li>需要持久保存的</li></ul></li><li>方案1：使用Python的set集合充当记录表？<ul><li>不可以的！因为set集合无法实现持久化存储</li></ul></li><li>方案2：使用redis的set集合充当记录表？<ul><li>可以的，因为redis的set既可以实现去重又可以进行数据的持久化存储。</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>基于两个场景实现增量式爬虫：</p><ul><li>场景1：如果爬取的数据都是存储在当前网页中，没有深度的数据爬取的必要。</li><li>场景2：爬取的数据存在于当前页和详情页中，具备深度爬取的必要。</li></ul></li><li><p>场景1的实现：</p><ul><li><p>数据指纹：</p><ul><li><p>数据的唯一标识。记录表中可以不直接存储数据本身，直接存储数据指纹更好一些。</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1>#爬虫文件</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>redis</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>..items</span> <span class=kn>import</span> <span class=n>Zlsdemo1ProItem</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>DuanziSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;duanzi&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://ishuo.cn/&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=c1>#Redis的链接对象</span>
</span></span><span class=line><span class=cl>      <span class=n>conn</span> <span class=o>=</span> <span class=n>redis</span><span class=o>.</span><span class=n>Redis</span><span class=p>(</span><span class=n>host</span><span class=o>=</span><span class=s1>&#39;127.0.0.1&#39;</span><span class=p>,</span><span class=n>port</span><span class=o>=</span><span class=mi>6379</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>li_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;list&#34;]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>content</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[1]/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./div[2]/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>all_data</span> <span class=o>=</span> <span class=n>title</span><span class=o>+</span><span class=n>content</span>
</span></span><span class=line><span class=cl>              <span class=c1>#生成该数据的数据指纹</span>
</span></span><span class=line><span class=cl>              <span class=kn>import</span> <span class=nn>hashlib</span>  <span class=c1># 导入一个生成数据指纹的模块</span>
</span></span><span class=line><span class=cl>              <span class=n>m</span> <span class=o>=</span> <span class=n>hashlib</span><span class=o>.</span><span class=n>md5</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>m</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>all_data</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>              <span class=n>data_id</span> <span class=o>=</span> <span class=n>m</span><span class=o>.</span><span class=n>hexdigest</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>              <span class=n>ex</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=o>.</span><span class=n>sadd</span><span class=p>(</span><span class=s1>&#39;data_id&#39;</span><span class=p>,</span><span class=n>data_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=k>if</span> <span class=n>ex</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span><span class=c1>#sadd执行成功（数据指纹在set集合中不存在）</span>
</span></span><span class=line><span class=cl>                  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;有最新数据的更新，正在爬取中......&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                  <span class=n>item</span> <span class=o>=</span> <span class=n>Zlsdemo1ProItem</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                  <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>                  <span class=n>item</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>content</span>
</span></span><span class=line><span class=cl>                  <span class=k>yield</span> <span class=n>item</span>
</span></span><span class=line><span class=cl>              <span class=k>else</span><span class=p>:</span><span class=c1>#sadd没有执行成功（数据指纹在set集合中存储）</span>
</span></span><span class=line><span class=cl>                  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;暂无最新数据更新，请等待......&#39;</span><span class=p>)</span>
</span></span></code></pre></div></li></ul></li></ul></li><li><p>场景2的实现：</p><ul><li><p>使用详情页的url充当数据指纹即可。</p></li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>scrapy</span>
</span></span><span class=line><span class=cl>  <span class=kn>import</span> <span class=nn>redis</span>
</span></span><span class=line><span class=cl>  <span class=kn>from</span> <span class=nn>..items</span> <span class=kn>import</span> <span class=n>Zlsdemo2ProItem</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>JianliSpider</span><span class=p>(</span><span class=n>scrapy</span><span class=o>.</span><span class=n>Spider</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;jianli&#39;</span>
</span></span><span class=line><span class=cl>      <span class=c1># allowed_domains = [&#39;www.xxx.com&#39;]</span>
</span></span><span class=line><span class=cl>      <span class=n>start_urls</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;https://sc.chinaz.com/jianli/free.html&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=n>conn</span> <span class=o>=</span> <span class=n>redis</span><span class=o>.</span><span class=n>Redis</span><span class=p>(</span><span class=n>host</span><span class=o>=</span><span class=s1>&#39;127.0.0.1&#39;</span><span class=p>,</span><span class=n>port</span><span class=o>=</span><span class=mi>6379</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>div_list</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;container&#34;]/div&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>div</span> <span class=ow>in</span> <span class=n>div_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>title</span> <span class=o>=</span> <span class=n>div</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./p/a/text()&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=c1>#充当数据指纹</span>
</span></span><span class=line><span class=cl>              <span class=n>detail_url</span> <span class=o>=</span> <span class=s1>&#39;https:&#39;</span><span class=o>+</span><span class=n>div</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./p/a/@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>ex</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conn</span><span class=o>.</span><span class=n>sadd</span><span class=p>(</span><span class=s1>&#39;data_id&#39;</span><span class=p>,</span><span class=n>detail_url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span> <span class=o>=</span> <span class=n>Zlsdemo2ProItem</span><span class=p>()</span>
</span></span><span class=line><span class=cl>              <span class=n>item</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>title</span>
</span></span><span class=line><span class=cl>              <span class=k>if</span> <span class=n>ex</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;有最新数据的更新，正在采集......&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                  <span class=k>yield</span> <span class=n>scrapy</span><span class=o>.</span><span class=n>Request</span><span class=p>(</span><span class=n>url</span><span class=o>=</span><span class=n>detail_url</span><span class=p>,</span><span class=n>callback</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>parse_detail</span><span class=p>,</span><span class=n>meta</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;item&#39;</span><span class=p>:</span><span class=n>item</span><span class=p>})</span>
</span></span><span class=line><span class=cl>              <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;暂无数据更新！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=k>def</span> <span class=nf>parse_detail</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>response</span><span class=p>):</span>
</span></span><span class=line><span class=cl>          <span class=n>item</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>meta</span><span class=p>[</span><span class=s1>&#39;item&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          <span class=n>download_url</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;down&#34;]/div[2]/ul/li[1]/a/@href&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>extract_first</span><span class=p>()</span>
</span></span><span class=line><span class=cl>          <span class=n>item</span><span class=p>[</span><span class=s1>&#39;download_url&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>download_url</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          <span class=k>yield</span> <span class=n>item</span>
</span></span></code></pre></div></li></ul></li></ul><h3 id=scrapy项目部署>scrapy项目部署<a hidden class=anchor aria-hidden=true href=#scrapy项目部署>#</a></h3><h4 id=scrapyd部署工具介绍>scrapyd部署工具介绍<a hidden class=anchor aria-hidden=true href=#scrapyd部署工具介绍>#</a></h4><ul><li>scrapyd是一个用于部署和运行scrapy爬虫的程序，它由 scrapy 官方提供的。它允许你通过JSON API来<strong>部署爬虫项目和控制爬虫运行</strong>。</li></ul><blockquote><p>所谓json api本质就是post请求的webapi</p></blockquote><ul><li>选择一台主机当做服务器，安装并启动 scrapyd 服务。再这之后，scrapyd 会以守护进程的方式存在系统中，监听爬虫地运行与请求，然后启动进程来执行爬虫程序。</li></ul><h4 id=环境安装>环境安装<a hidden class=anchor aria-hidden=true href=#环境安装>#</a></h4><ul><li>scrapyd服务:</li></ul><p>​ <code>pip install scrapyd</code></p><ul><li>scrapyd客户端:</li></ul><p>​ <code>pip install scrapyd-client</code></p><p>​ 一定要安装较新的版本10以上的版本，如果是现在安装的一般都是新版本</p><h4 id=启动scrapyd服务>启动scrapyd服务<a hidden class=anchor aria-hidden=true href=#启动scrapyd服务>#</a></h4><ul><li>打开终端<strong>在scrapy项目路径下</strong> 启动scrapyd的命令： <code>scrapyd</code></li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/canw0916/picgo_imgs@main/blog/Snip20220308_13.png alt=Snip20220308_13></p><ul><li>scrapyd 也提供了 web 的接口。方便我们查看和管理爬虫程序。默认情况下 scrapyd 监听 6800 端口，运行 scrapyd 后。在本机上使用浏览器访问 <code>http://localhost:6800/</code>地址即可查看到当前可以运行的项目。</li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/canw0916/picgo_imgs@main/blog/Snip20220308_14.png alt=Snip20220308_14></p><ul><li>点击job可以查看任务监控界面</li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/canw0916/picgo_imgs@main/blog/Snip20220308_16.png alt=Snip20220308_16></p><h4 id=scrapy项目部署-1>scrapy项目部署<a hidden class=anchor aria-hidden=true href=#scrapy项目部署-1>#</a></h4><h5 id=配置需要部署的项目>配置需要部署的项目<a hidden class=anchor aria-hidden=true href=#配置需要部署的项目>#</a></h5><ul><li>编辑需要部署的项目的scrapy.cfg文件(需要将哪一个爬虫部署到scrapyd中，就配置该项目的该文件)</li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/canw0916/picgo_imgs@main/blog/Snip20220308_17.png alt=Snip20220308_17></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[deploy:部署名(部署名可以自行定义)] 
</span></span><span class=line><span class=cl>url = http://localhost:6800/ 
</span></span><span class=line><span class=cl>project = 项目名(创建爬虫项目时使用的名称)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>username = bobo # 如果不需要用户名可以不写
</span></span><span class=line><span class=cl>password = 123456 # 如果不需要密码可以不写
</span></span></code></pre></div><h5 id=部署项目到scrapyd>部署项目到scrapyd<a hidden class=anchor aria-hidden=true href=#部署项目到scrapyd>#</a></h5><ul><li><p>同样在<strong>scrapy项目路径下</strong>执行如下指令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>scrapyd-deploy 部署名(配置文件中设置的名称) -p 项目名称
</span></span></code></pre></div></li><li><p>部署成功之后就可以看到部署的项目</p></li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/canw0916/picgo_imgs@main/blog/Snip20220308_18.png alt=Snip20220308_18></p><ul><li><p>使用以下命令检查部署爬虫结果：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>  scrapyd-deploy -L 部署名
</span></span></code></pre></div></li></ul></li></ul><h4 id=管理scrapy项目>管理scrapy项目<a hidden class=anchor aria-hidden=true href=#管理scrapy项目>#</a></h4><h5 id=指令管理>指令管理<a hidden class=anchor aria-hidden=true href=#指令管理>#</a></h5><ul><li><p>安装curl命令行工具</p><ul><li>window需要安装</li><li>linux和mac无需单独安装</li></ul></li><li><p>window安装步骤：</p><ul><li>下载curl文件：https://curl.se/download.html，打开网页后向下拖动，找到window系统对应版本下载</li></ul><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/canw0916/picgo_imgs@main/blog/Snip20220308_19.png alt=Snip20220308_19></p><ul><li>下载后，放置到一个无中文的文件夹下直接解压缩，解压后将bin文件夹配置环境变量！</li><li>参考网页：https://www.cnblogs.com/lisa2016/p/12193494.html</li></ul></li><li><p>启动项目：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>curl http://localhost:6800/schedule.json -d project=项目名 -d spider=爬虫名
</span></span></code></pre></div><ul><li><p>返回结果：注意期中的jobid，在关闭项目时候会用到</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  {&#34;status&#34;: &#34;ok&#34;, &#34;jobid&#34;: &#34;94bd8ce041fd11e6af1a000c2969bafd&#34;, &#34;node_name&#34;: &#34;james-virtual-machine&#34;}
</span></span></code></pre></div></li></ul></li></ul></li><li><p>关闭项目：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  curl http://localhost:6800/cancel.json -d project=项目名 -d job=项目的jobid
</span></span></code></pre></div></li></ul></li><li><p>删除爬虫项目：</p><ul><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  curl http://localhost:6800/delproject.json -d project=爬虫项目名称
</span></span></code></pre></div></li></ul></li></ul><h5 id=requests模块控制scrapy项目>requests模块控制scrapy项目<a hidden class=anchor aria-hidden=true href=#requests模块控制scrapy项目>#</a></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 启动爬虫</span>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s1>&#39;http://localhost:6800/schedule.json&#39;</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=s1>&#39;project&#39;</span><span class=p>:</span> <span class=n>项目名</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=s1>&#39;spider&#39;</span><span class=p>:</span> <span class=n>爬虫名</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>resp</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 停止爬虫</span>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s1>&#39;http://localhost:6800/cancel.json&#39;</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=s1>&#39;project&#39;</span><span class=p>:</span> <span class=n>项目名</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=s1>&#39;job&#39;</span><span class=p>:</span> <span class=n>启动爬虫时返回的jobid</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>resp</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=生产者消费者模式>生产者消费者模式<a hidden class=anchor aria-hidden=true href=#生产者消费者模式>#</a></h3><h4 id=认识生产者和消费者模式>认识生产者和消费者模式<a hidden class=anchor aria-hidden=true href=#认识生产者和消费者模式>#</a></h4><p>生产者和消费者是异步爬虫中很常见的一个问题。产生数据的模块，我们称之为生产者，而处理数据的模块，就称为消费者。</p><p>例如：</p><p>​ 图片数据爬取中，解析出图片链接的操作就是在生产数据</p><p>​ 对图片链接发起请求下载图片的操作就是在消费数据</p><h4 id=为什么要使用生产者和消费者模式>为什么要使用生产者和消费者模式<a hidden class=anchor aria-hidden=true href=#为什么要使用生产者和消费者模式>#</a></h4><blockquote><p>​ 在异步世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>threading</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lxml</span> <span class=kn>import</span> <span class=n>etree</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>queue</span> <span class=kn>import</span> <span class=n>Queue</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>urllib.request</span> <span class=kn>import</span> <span class=n>urlretrieve</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>time</span> <span class=kn>import</span> <span class=n>sleep</span>
</span></span><span class=line><span class=cl><span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;User-Agent&#34;</span><span class=p>:</span> <span class=s2>&#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#生产数据：解析提取图片地址</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Producer</span><span class=p>(</span><span class=n>threading</span><span class=o>.</span><span class=n>Thread</span><span class=p>):</span><span class=c1>#生产者线程</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>page_queue</span><span class=p>,</span><span class=n>img_queue</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>page_queue</span> <span class=o>=</span> <span class=n>page_queue</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>img_queue</span> <span class=o>=</span> <span class=n>img_queue</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>page_queue</span><span class=o>.</span><span class=n>empty</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Producer任务结束&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>break</span>
</span></span><span class=line><span class=cl>            <span class=c1>#从page_queue中取出一个页码链接</span>
</span></span><span class=line><span class=cl>            <span class=n>url</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>page_queue</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=c1>#从当前的页码对应的页面中解析出更多的图片地址</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>parse_detail</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>parse_detail</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>url</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span><span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span><span class=o>.</span><span class=n>encoding</span> <span class=o>=</span> <span class=s1>&#39;gbk&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>page_text</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>        <span class=n>tree</span> <span class=o>=</span> <span class=n>etree</span><span class=o>.</span><span class=n>HTML</span><span class=p>(</span><span class=n>page_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>li_list</span> <span class=o>=</span> <span class=n>tree</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;//*[@id=&#34;main&#34;]/div[3]/ul/li&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>li</span> <span class=ow>in</span> <span class=n>li_list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>img_src</span> <span class=o>=</span> <span class=s1>&#39;https://pic.netbian.com&#39;</span><span class=o>+</span><span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./a/img/@src&#39;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>img_title</span> <span class=o>=</span> <span class=n>li</span><span class=o>.</span><span class=n>xpath</span><span class=p>(</span><span class=s1>&#39;./a/b/text()&#39;</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>+</span><span class=s1>&#39;.jpg&#39;</span>
</span></span><span class=line><span class=cl>            <span class=n>dic</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s1>&#39;title&#39;</span><span class=p>:</span><span class=n>img_title</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s1>&#39;src&#39;</span><span class=p>:</span><span class=n>img_src</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>img_queue</span><span class=o>.</span><span class=n>put</span><span class=p>(</span><span class=n>dic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#消费数据：对图片地址进行数据请求</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Consumer</span><span class=p>(</span><span class=n>threading</span><span class=o>.</span><span class=n>Thread</span><span class=p>):</span><span class=c1>#消费者线程</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>page_queue</span><span class=p>,</span><span class=n>img_queue</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>page_queue</span> <span class=o>=</span> <span class=n>page_queue</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>img_queue</span> <span class=o>=</span> <span class=n>img_queue</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>img_queue</span><span class=o>.</span><span class=n>empty</span><span class=p>()</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>page_queue</span><span class=o>.</span><span class=n>empty</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Consumer任务结束&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>break</span>
</span></span><span class=line><span class=cl>            <span class=n>dic</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>img_queue</span><span class=o>.</span><span class=n>get</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>title</span> <span class=o>=</span> <span class=n>dic</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>src</span> <span class=o>=</span> <span class=n>dic</span><span class=p>[</span><span class=s1>&#39;src&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>urlretrieve</span><span class=p>(</span><span class=n>src</span><span class=p>,</span><span class=s1>&#39;imgs/&#39;</span><span class=o>+</span><span class=n>title</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=n>title</span><span class=p>,</span><span class=s1>&#39;下载完毕！&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1>#该队列中存储即将要要去的页面页码链接</span>
</span></span><span class=line><span class=cl>    <span class=n>page_queue</span> <span class=o>=</span> <span class=n>Queue</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#该队列存储生产者生产出来的图片地址</span>
</span></span><span class=line><span class=cl>    <span class=n>img_queue</span> <span class=o>=</span> <span class=n>Queue</span><span class=p>(</span><span class=mi>60</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#该循环可以将2，3，4这三个页码链接放入page_queue中</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>url</span> <span class=o>=</span> <span class=s1>&#39;https://pic.netbian.com/4kmeinv/index_</span><span class=si>%d</span><span class=s1>.html&#39;</span><span class=o>%</span><span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>page_queue</span><span class=o>.</span><span class=n>put</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#生产者</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>Producer</span><span class=p>(</span><span class=n>page_queue</span><span class=p>,</span><span class=n>img_queue</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span><span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1>#消费者</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>Consumer</span><span class=p>(</span><span class=n>page_queue</span><span class=p>,</span><span class=n>img_queue</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span><span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>main</span><span class=p>()</span>
</span></span></code></pre></div></div><div class=post-reward><div style=padding:0;margin:0;width:100%;font-size:16px;text-align:center><div id=QR style=opacity:0><div id=wechat style=display:inline-block><a class=fancybox rel=group><img id=wechat_qr src=https://canw0916.github.io/img/wxPay.jpg alt=wechat_pay></a><p>微信</p></div><div id=alipay style=display:inline-block><a class=fancybox rel=group><img id=alipay_qr src=https://canw0916.github.io/img/aliPay.jpg alt=alipay></a><p>支付宝</p></div></div><button id=rewardButton onclick='var qr=document.getElementById("QR");qr.style.opacity==="0"?qr.style.opacity="1":qr.style.opacity="0"'>
<span>🧧 鼓励</span></button></div></div><footer class=post-footer><nav class=paginav><a class=prev href=https://canw0916.github.io/en/posts/tech/markdown%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/><span class=title>« 上一页</span><br><span>Markdown快速入门</span>
</a><a class=next href=https://canw0916.github.io/en/posts/tech/mongodb/><span class=title>下一页 »</span><br><span>MongoDB</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy框架 on twitter" href="https://twitter.com/intent/tweet/?text=Scrapy%e6%a1%86%e6%9e%b6&amp;url=https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy框架 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f&amp;title=Scrapy%e6%a1%86%e6%9e%b6&amp;summary=Scrapy%e6%a1%86%e6%9e%b6&amp;source=https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy框架 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f&title=Scrapy%e6%a1%86%e6%9e%b6"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy框架 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy框架 on whatsapp" href="https://api.whatsapp.com/send?text=Scrapy%e6%a1%86%e6%9e%b6%20-%20https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Scrapy框架 on telegram" href="https://telegram.me/share/url?text=Scrapy%e6%a1%86%e6%9e%b6&amp;url=https%3a%2f%2fcanw0916.github.io%2fen%2fposts%2ftech%2fscrapy%25E6%25A1%2586%25E6%259E%25B6%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></div><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>💬评论</span><hr></div><div id=tcomment></div><script src=https://utteranc.es/client.js repo=canw0916/utterances_comments issue-term=title theme=photon-dark crossorigin=anonymous async></script></div></article></main><script async src=https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js></script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><footer class=footer><span>Copyright
&copy;
2023-2025
<a href=https://canw0916.github.io/en/ style=color:#939393>Felix's Blog</a>
All Rights Reserved
</span><a href=https://beian.miit.gov.cn/ target=_blank style=color:#939393></a>&nbsp;
<span><a target=_blank href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=null" style=display:inline-block;text-decoration:none;height:20px;color:#939393><img src style="float:left;margin:0 5px 0 0">
</a></span><span id=busuanzi_container><link rel=stylesheet href=//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css>总访客数: <span id=busuanzi_value_site_uv></span>
总访问量: <span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>let mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>400||document.documentElement.scrollTop>400?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="📄复制";function s(){t.innerText="👌🏻已复制!",setTimeout(()=>{t.innerText="📄复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){let t=e.textContent+`\r
————————————————\r
版权声明：本文为「Felix's Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\r
原文链接：`+location.href;navigator.clipboard.writeText(t),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild===n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName==="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script src=https://code.jquery.com/jquery-1.12.4.min.js></script><script>$("code[class^=language] ").on("mouseover",function(){this.clientWidth<this.scrollWidth&&$(this).css("width","135%")}).on("mouseout",function(){$(this).css("width","100%")})</script></body></html>